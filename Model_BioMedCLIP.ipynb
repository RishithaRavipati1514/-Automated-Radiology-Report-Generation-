{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ufWnyFPjfpYz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufWnyFPjfpYz",
        "outputId": "78ce5af9-1b9e-4ddf-9c56-2fdc4c9a2e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to uninstall and reinstall pycocoevalcap to resolve potential corruption...\n",
            "Found existing installation: pycocoevalcap 1.2\n",
            "Uninstalling pycocoevalcap-1.2:\n",
            "  Successfully uninstalled pycocoevalcap-1.2\n",
            "Collecting pycocoevalcap\n",
            "  Using cached pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.0.2)\n",
            "Using cached pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "Installing collected packages: pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (2.0.2)\n",
            "\n",
            "pycocoevalcap reinstallation attempt complete.\n",
            "Successfully imported COCOEvalCap from pycocoevalcap.eval\n",
            "NLTK data path added: /usr/local/nltk_data\n",
            "Attempting to download essential NLTK data to the specified path...\n",
            " 'punkt' found. No re-download needed.\n",
            " 'wordnet' not found, attempting download...\n",
            " 'wordnet' download attempt complete.\n",
            " 'averaged_perceptron_tagger' not found, attempting download...\n",
            " 'averaged_perceptron_tagger' download attempt complete.\n",
            " 'omw-1.4' not found, attempting download...\n",
            " 'omw-1.4' download attempt complete.\n",
            "Checking for and downloading 'punkt_tab' if needed...\n",
            " 'punkt_tab' not found, attempting download...\n",
            " 'punkt_tab' download attempt complete.\n",
            "All NLTK data downloads/checks complete.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted.\n",
            "Running on device: cuda\n",
            "CUDA Device: Tesla T4\n",
            "\n",
            "Initial setup and configuration complete with updated hyperparameters.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Initial Setup, Drive Mount, and Configuration\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Library Installations\n",
        "# ==============================================================================\n",
        "# Use -q for quiet installation.\n",
        "# Ensure all necessary libraries are listed.\n",
        "!pip install open_clip_torch==2.23.0 transformers==4.35.2 matplotlib scikit-learn nltk rouge_score pycocotools pycocoevalcap gradio -q\n",
        "\n",
        "# It's good practice to ensure pycocoevalcap is correctly installed.\n",
        "# Uninstalling and reinstalling can fix some environment issues.\n",
        "print(\"Attempting to uninstall and reinstall pycocoevalcap to resolve potential corruption...\")\n",
        "!pip uninstall pycocoevalcap -y # -y for yes to prompts\n",
        "!pip install pycocoevalcap\n",
        "!pip install pycocotools\n",
        "print(\"\\npycocoevalcap reinstallation attempt complete.\")\n",
        "\n",
        "# --- REMOVING MANUAL WGET FOR EVAL.PY ---\n",
        "# The previous approach of downloading eval.py manually often leads to naming\n",
        "# conflicts or incorrect versions/structures. We will rely on the installed package.\n",
        "# !wget https://raw.githubusercontent.com/tylin/coco-caption/master/pycocoevalcap/eval.py -O eval.py\n",
        "# print(\"eval.py manual download attempt removed. Relying on package install.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Core Imports\n",
        "# ==============================================================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import collections\n",
        "import itertools\n",
        "from pycocotools.coco import COCO\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import nltk\n",
        "import torch.nn.functional as F # Ensure F is imported for functional operations like log_softmax\n",
        "\n",
        "# Attempt to import COCOEval or COCOEvalCap from the installed pycocoevalcap package.\n",
        "# This is the standard way it should be done.\n",
        "try:\n",
        "    from pycocoevalcap.eval import COCOEval\n",
        "    print(\"Successfully imported COCOEval from pycocoevalcap.eval\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        from pycocoevalcap.eval import COCOEvalCap\n",
        "        print(\"Successfully imported COCOEvalCap from pycocoevalcap.eval\")\n",
        "    except ImportError as e:\n",
        "        print(f\"Failed to import COCOEval or COCOEvalCap from pycocoevalcap.eval: {e}\")\n",
        "        print(\"Please ensure pycocoevalcap is correctly installed and its structure supports direct import.\")\n",
        "        # If this point is reached, the environment/package is severely misconfigured or incompatible.\n",
        "        # You might need to:\n",
        "        # 1. Check the exact version of pycocoevalcap installed and its documentation.\n",
        "        # 2. Consider installing an older, known-good version if encountering constant issues.\n",
        "        #    e.g., !pip install pycocoevalcap==1.0 (or similar)\n",
        "        # 3. Manually inspect the 'eval.py' within '/usr/local/lib/python3.11/dist-packages/pycocoevalcap/'\n",
        "        #    to see what classes are actually defined there.\n",
        "        # !ls /usr/local/lib/python3.11/dist-packages/pycocoevalcap/\n",
        "        # !cat /usr/local/lib/python3.11/dist-packages/pycocoevalcap/eval.py | grep \"class \"\n",
        "        pass # Allow the script to continue to see other errors, but know this is unresolved.\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. NLTK Data Downloads\n",
        "# ==============================================================================\n",
        "NLTK_DATA_PATH = '/usr/local/nltk_data'\n",
        "os.makedirs(NLTK_DATA_PATH, exist_ok=True)\n",
        "nltk.data.path.append(NLTK_DATA_PATH)\n",
        "\n",
        "print(f\"NLTK data path added: {NLTK_DATA_PATH}\")\n",
        "print(\"Attempting to download essential NLTK data to the specified path...\")\n",
        "\n",
        "# List all standard NLTK downloads clearly\n",
        "required_nltk_data = ['punkt', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4'] # 'omw-1.4' often needed for wordnet\n",
        "for data_pkg in required_nltk_data:\n",
        "    try:\n",
        "        nltk.data.find(f'tokenizers/{data_pkg}' if data_pkg == 'punkt' else f'corpora/{data_pkg}')\n",
        "        print(f\" '{data_pkg}' found. No re-download needed.\")\n",
        "    except LookupError:\n",
        "        print(f\" '{data_pkg}' not found, attempting download...\")\n",
        "        nltk.download(data_pkg, download_dir=NLTK_DATA_PATH, quiet=True)\n",
        "        print(f\" '{data_pkg}' download attempt complete.\")\n",
        "\n",
        "# Specific check for punkt_tab as it caused issues before\n",
        "print(\"Checking for and downloading 'punkt_tab' if needed...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab/english.pickle') # Specific file within punkt_tab\n",
        "except LookupError:\n",
        "    print(\" 'punkt_tab' not found, attempting download...\")\n",
        "    nltk.download('punkt_tab', download_dir=NLTK_DATA_PATH, quiet=True)\n",
        "    print(\" 'punkt_tab' download attempt complete.\")\n",
        "else:\n",
        "    print(\" 'punkt_tab' found. No re-download needed.\")\n",
        "\n",
        "print(\"All NLTK data downloads/checks complete.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Google Drive Mount\n",
        "# ==============================================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Device Configuration\n",
        "# ==============================================================================\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Running on device: {DEVICE}\")\n",
        "if DEVICE == 'cuda':\n",
        "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
        "    # Optional: Set a specific CUDA device if you have multiple GPUs\n",
        "    # torch.cuda.set_device(0)\n",
        "    # torch.backends.cudnn.benchmark = True # Can improve performance if input sizes are constant\n",
        "else:\n",
        "    print(\"No GPU available, running on CPU. Training will be slower.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. Path Configurations\n",
        "# ==============================================================================\n",
        "MODEL_IDENTIFIER_FOR_OUTPUT = \"biomedclip_vitb16\"\n",
        "\n",
        "TRAIN_PKL_PATH = f\"/content/drive/My Drive/Colab Notebooks/Final_Train_Data_with_{MODEL_IDENTIFIER_FOR_OUTPUT}_features.pkl\"\n",
        "VAL_PKL_PATH = f\"/content/drive/My Drive/Colab Notebooks/Final_CV_Data_with_{MODEL_IDENTIFIER_FOR_OUTPUT}_features.pkl\"\n",
        "TEST_PKL_PATH = f\"/content/drive/My Drive/Colab Notebooks/Final_Test_Data_with_{MODEL_IDENTIFIER_FOR_OUTPUT}_features.pkl\"\n",
        "\n",
        "# IMAGE_BASE_DIR: This should be the directory that CONTAINS the 'NLMCXR_png' folder.\n",
        "# From your previous output, Image ID was `NLMCXR_png/CXR1472_IM-0305_0`.\n",
        "# If `IMAGE_BASE_DIR` points to the parent of `NLMCXR_png`, then os.path.join works directly.\n",
        "# E.g., if images are at `/content/drive/My Drive/medical_data/NLMCXR_png/img.png`\n",
        "# and the dataframe `Person_id` is `NLMCXR_png/img`, then IMAGE_BASE_DIR should be `/content/drive/My Drive/medical_data/`\n",
        "IMAGE_BASE_DIR = \"/content/drive/My Drive/\" # Assuming 'NLMCXR_png' is directly under My Drive.\n",
        "                                             # ADJUST THIS IF YOUR 'NLMCXR_png' IS IN A SUBFOLDER (e.g., Colab Notebooks)\n",
        "                                             # Example: If your structure is `My Drive/Colab Notebooks/NLMCXR_png`\n",
        "                                             # then IMAGE_BASE_DIR should be \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "\n",
        "# Model saving paths\n",
        "MODEL_SAVE_DIR = \"/content/drive/My Drive/Colab Notebooks/models\"\n",
        "MODEL_SAVE_PREFIX = f\"{MODEL_IDENTIFIER_FOR_OUTPUT}_captioning_model_v2\"\n",
        "MODEL_SAVE_PATH_ENCODER = os.path.join(MODEL_SAVE_DIR, f\"{MODEL_SAVE_PREFIX}_encoder.pth\")\n",
        "MODEL_SAVE_PATH_DECODER = os.path.join(MODEL_SAVE_DIR, f\"{MODEL_SAVE_PREFIX}_decoder.pth\")\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True) # Ensure the directory exists\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. Model Hyperparameters (for tuning)\n",
        "# ==============================================================================\n",
        "# Keep these together for easy modification\n",
        "EMBED_SIZE = 768            # Dimension of word embeddings and encoder output (output of EncoderCNN)\n",
        "HIDDEN_SIZE = 1536          # Hidden state dimension of LSTM\n",
        "NUM_LAYERS = 3              # Number of LSTM layers\n",
        "DROPOUT = 0.3               # Dropout rate for regularization (can be tuned 0.3-0.5 is common)\n",
        "LEARNING_RATE = 0.0002     # Adam optimizer learning rate (critical for stability)\n",
        "NUM_EPOCHS = 25            # Number of training epochs\n",
        "BATCH_SIZE = 32             # Batch size for DataLoader\n",
        "\n",
        "# NEW: Attention Dimension - often set to HIDDEN_SIZE or EMBED_SIZE\n",
        "ATTENTION_DIM = HIDDEN_SIZE # Crucial for the attention mechanism in DecoderRNN\n",
        "\n",
        "# Corrected ORIGINAL_FEATURE_DIM\n",
        "# If you are concatenating two 1024-dim features (e.g., from two different views/projections),\n",
        "# then the input dimension to EncoderCNN's linear layer must be 2048.\n",
        "# If you are using a single 1024-dim feature, then set to 1024.\n",
        "# Based on your previous context (\"concatenating two 1024-dim features\"), it should be 2048.\n",
        "ORIGINAL_FEATURE_DIM = 1024 # <--- CRITICAL FIX: Set to 2048 if concatenating two 1024-dim features\n",
        "\n",
        "# MAX_SEQ_LEN is critical and should be chosen based on your dataset's caption lengths.\n",
        "# It represents the maximum length of a *padded* caption including <start> and <end>.\n",
        "# If your longest report is 70 words, then 70 + 2 (start/end) = 72, so MAX_SEQ_LEN=75 or 80 is reasonable.\n",
        "# If reports can be much longer, adjust this.\n",
        "MAX_SEQ_LEN = 100 # Example: Set this after analyzing your report length distribution.\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. Tokenization and Vocabulary Constants\n",
        "# ==============================================================================\n",
        "# Define special tokens\n",
        "START_TOKEN = '<start>'\n",
        "END_TOKEN = '<end>'\n",
        "PAD_TOKEN = '<pad>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "# Minimum frequency for a word to be included in the vocabulary\n",
        "MIN_WORD_FREQ = 3 # Good balance for reducing vocab size while keeping important words.\n",
        "                  # Can be tuned: lower if you need to capture more rare medical terms,\n",
        "                  # but increases vocab size and complexity.\n",
        "\n",
        "# ==============================================================================\n",
        "# 9. Logging Configuration\n",
        "# ==============================================================================\n",
        "# How often to print training loss (every N batches)\n",
        "LOG_INTERVAL = 100\n",
        "# Gradient clipping value\n",
        "GRADIENT_CLIP = 5.0 # Common value for RNNs to prevent exploding gradients\n",
        "\n",
        "print(\"\\nInitial setup and configuration complete with updated hyperparameters.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mqU4OVxZYspX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqU4OVxZYspX",
        "outputId": "4a196660-d191-40d0-836d-f5b56d9c1e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-processed dataframes with image features...\n",
            "Dataframes loaded successfully.\n",
            "Train samples: 2764, Val samples: 609, Test samples: 377\n",
            "Using MIN_WORD_FREQ for vocabulary: 3\n",
            "\n",
            "Vocabulary size: 822\n",
            "PAD_TOKEN index: 0\n",
            "UNK_TOKEN index: 1\n",
            "START_TOKEN index: 2\n",
            "END_TOKEN index: 3\n",
            "Vocabulary saved successfully to /content/drive/My Drive/Colab Notebooks/data/vocab.pkl\n",
            "\n",
            "Maximum sequence length determined dynamically (95th percentile + 5): 64\n",
            "Diagnostic: First valid row - Shape of img1: (512,)\n",
            "Diagnostic: First valid row - Shape of img2: (512,)\n",
            "Diagnostic: First valid row - Shape after concat: (1024,)\n",
            "Diagnostic: First valid row - Shape of img1: (512,)\n",
            "Diagnostic: First valid row - Shape of img2: (512,)\n",
            "Diagnostic: First valid row - Shape after concat: (1024,)\n",
            "Diagnostic: First valid row - Shape of img1: (512,)\n",
            "Diagnostic: First valid row - Shape of img2: (512,)\n",
            "Diagnostic: First valid row - Shape after concat: (1024,)\n",
            "\n",
            "Text preprocessing and dataset creation complete. DataLoaders ready.\n",
            "First training batch Image features shape: torch.Size([32, 1024])\n",
            "First training batch Report sequences shape: torch.Size([32, 64])\n",
            "First training batch Report lengths shape: torch.Size([32])\n",
            "Encoder model not yet instantiated, cannot check its expected input dimension.\n",
            "\n",
            "--- Diagnostic: DataFrame Column Check ---\n",
            "Columns in train_df: ['Person_id', 'Image1', 'Image2', 'Report', 'image_feat_img1', 'image_feat_img2', 'Report_Seq']\n",
            "Columns in val_df: ['Person_id', 'Image1', 'Image2', 'Report', 'image_feat_img1', 'image_feat_img2', 'Report_Seq']\n",
            "Columns in test_df: ['Person_id', 'Image1', 'Image2', 'Report', 'image_feat_img1', 'image_feat_img2', 'Report_Seq']\n",
            "------------------------------------------\n",
            "\n",
            "--- Diagnostic: Head of test_df ---\n",
            "                           Person_id  \\\n",
            "0       NLMCXR_png/CXR1480_IM-0311_0   \n",
            "1       NLMCXR_png/CXR1498_IM-0322_0   \n",
            "2       NLMCXR_png/CXR1467_IM-0302_0   \n",
            "3  NLMCXR_png/CXR1478_IM-0310-0001_0   \n",
            "4  NLMCXR_png/CXR1478_IM-0310-0001_1   \n",
            "\n",
            "                                     Image1  \\\n",
            "0       NLMCXR_png/CXR1480_IM-0311-1001.png   \n",
            "1       NLMCXR_png/CXR1498_IM-0322-1001.png   \n",
            "2       NLMCXR_png/CXR1467_IM-0302-1001.png   \n",
            "3  NLMCXR_png/CXR1478_IM-0310-0001-0001.png   \n",
            "4  NLMCXR_png/CXR1478_IM-0310-0001-0001.png   \n",
            "\n",
            "                                     Image2  \\\n",
            "0       NLMCXR_png/CXR1480_IM-0311-2001.png   \n",
            "1       NLMCXR_png/CXR1498_IM-0322-2001.png   \n",
            "2       NLMCXR_png/CXR1467_IM-0302-2001.png   \n",
            "3  NLMCXR_png/CXR1478_IM-0310-0001-0002.png   \n",
            "4  NLMCXR_png/CXR1478_IM-0310-0001-0003.png   \n",
            "\n",
            "                                              Report  \\\n",
            "0  startseq the heart normal size and contour . t...   \n",
            "1  startseq apical lordotic frontal view . heart ...   \n",
            "2  startseq the lungs are clear bilaterally . spe...   \n",
            "3  startseq stable cardiomediastinal silhouette w...   \n",
            "4  startseq stable cardiomediastinal silhouette w...   \n",
            "\n",
            "                                     image_feat_img1  \\\n",
            "0  [-0.16752976, -0.010210142, 1.5963812, 1.57990...   \n",
            "1  [-0.9516784, 0.74023855, -0.1969476, 0.1697585...   \n",
            "2  [-0.92477393, 0.23230523, 0.9018729, 1.20064, ...   \n",
            "3  [-0.41435784, 0.21622048, 1.2377374, 1.7472956...   \n",
            "4  [-0.41435784, 0.21622048, 1.2377374, 1.7472956...   \n",
            "\n",
            "                                     image_feat_img2  \\\n",
            "0  [-0.66824985, 0.42360482, 0.57186955, 1.354056...   \n",
            "1  [-1.0237719, 0.5014529, -0.072926134, -0.03273...   \n",
            "2  [-1.1234759, 0.7946276, 0.44976148, 0.23233041...   \n",
            "3  [-0.7549354, 0.090218335, 0.7791156, 1.2165921...   \n",
            "4  [-0.3249845, 0.07592104, 1.3137302, 1.5965399,...   \n",
            "\n",
            "                                          Report_Seq  \n",
            "0  [2, 10, 5, 16, 8, 17, 11, 43, 4, 14, 6, 29, 22...  \n",
            "1  [2, 10, 249, 672, 112, 118, 4, 16, 17, 20, 8, ...  \n",
            "2  [2, 10, 5, 18, 7, 22, 72, 4, 171, 6, 60, 19, 2...  \n",
            "3  [2, 10, 33, 28, 25, 34, 8, 16, 17, 29, 153, 15...  \n",
            "4  [2, 10, 33, 28, 25, 34, 8, 16, 17, 29, 153, 15...  \n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 2: Load Feature Data and Text Preprocessing - REVISED VOCAB CREATION (with refined lengths calculation)\n",
        "\n",
        "# Cell 2: Data Loading, Preprocessing, and Dataset/DataLoader Setup\n",
        "\n",
        "print(\"Loading pre-processed dataframes with image features...\")\n",
        "try:\n",
        "    train_df = pd.read_pickle(TRAIN_PKL_PATH)\n",
        "    val_df = pd.read_pickle(VAL_PKL_PATH)\n",
        "    test_df = pd.read_pickle(TEST_PKL_PATH)\n",
        "    print(\"Dataframes loaded successfully.\")\n",
        "    print(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}, Test samples: {len(test_df)}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Feature .pkl files not found. Please ensure Cell 5 of the feature extraction notebook completed successfully.\")\n",
        "    print(f\"Missing file: {e.filename}\")\n",
        "    raise # Stop execution if files aren't found\n",
        "\n",
        "# --- Text Preprocessing ---\n",
        "all_train_reports = train_df['Report'].dropna().tolist()\n",
        "tokenized_reports = [[START_TOKEN] + nltk.word_tokenize(report.lower()) + [END_TOKEN] for report in all_train_reports]\n",
        "word_counts = collections.Counter(itertools.chain(*tokenized_reports))\n",
        "print(f\"Using MIN_WORD_FREQ for vocabulary: {MIN_WORD_FREQ}\")\n",
        "\n",
        "vocab = {\n",
        "    PAD_TOKEN: 0,\n",
        "    UNK_TOKEN: 1,\n",
        "    START_TOKEN: 2,\n",
        "    END_TOKEN: 3\n",
        "}\n",
        "current_idx = len(vocab)\n",
        "filtered_words_for_vocab = sorted([word for word, count in word_counts.items()\n",
        "                                   if count >= MIN_WORD_FREQ and word not in vocab],\n",
        "                                   key=lambda x: (-word_counts[x], x))\n",
        "\n",
        "for word in filtered_words_for_vocab:\n",
        "    vocab[word] = current_idx\n",
        "    current_idx += 1\n",
        "\n",
        "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
        "VOCAB_SIZE = len(vocab)\n",
        "PAD_TOKEN_INDEX = vocab[PAD_TOKEN]\n",
        "START_TOKEN_INDEX = vocab[START_TOKEN]\n",
        "END_TOKEN_INDEX = vocab[END_TOKEN]\n",
        "UNK_TOKEN_INDEX = vocab[UNK_TOKEN]\n",
        "\n",
        "print(f\"\\nVocabulary size: {VOCAB_SIZE}\")\n",
        "print(f\"PAD_TOKEN index: {PAD_TOKEN_INDEX}\")\n",
        "print(f\"UNK_TOKEN index: {UNK_TOKEN_INDEX}\")\n",
        "print(f\"START_TOKEN index: {START_TOKEN_INDEX}\")\n",
        "print(f\"END_TOKEN index: {END_TOKEN_INDEX}\")\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "BASE_PATH = '/content/drive/My Drive/Colab Notebooks'\n",
        "VOCAB_SAVE_DIR = os.path.join(BASE_PATH, 'data')\n",
        "VOCAB_SAVE_PATH = os.path.join(VOCAB_SAVE_DIR, 'vocab.pkl')\n",
        "os.makedirs(VOCAB_SAVE_DIR, exist_ok=True)\n",
        "with open(VOCAB_SAVE_PATH, 'wb') as f:\n",
        "    pickle.dump(vocab, f)\n",
        "print(f\"Vocabulary saved successfully to {VOCAB_SAVE_PATH}\")\n",
        "\n",
        "def text_to_sequence(text, vocab, max_seq_len, unk_idx, pad_idx, start_idx, end_idx):\n",
        "    tokens = [START_TOKEN] + nltk.word_tokenize(text.lower()) + [END_TOKEN]\n",
        "    sequence = [vocab.get(token, unk_idx) for token in tokens]\n",
        "    if len(sequence) < max_seq_len:\n",
        "        sequence.extend([pad_idx] * (max_seq_len - len(sequence)))\n",
        "    else:\n",
        "        sequence = sequence[:max_seq_len]\n",
        "    return sequence\n",
        "\n",
        "# Dynamically determine MAX_SEQ_LEN\n",
        "all_report_lengths_incl_special = [len(nltk.word_tokenize(report.lower())) + 2 for report in all_train_reports]\n",
        "MAX_SEQ_LEN = int(np.percentile(all_report_lengths_incl_special, 95)) + 5\n",
        "print(f\"\\nMaximum sequence length determined dynamically (95th percentile + 5): {MAX_SEQ_LEN}\")\n",
        "\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    df['Report_Seq'] = df['Report'].astype(str).apply(\n",
        "        lambda x: text_to_sequence(x, vocab, MAX_SEQ_LEN, UNK_TOKEN_INDEX, PAD_TOKEN_INDEX, START_TOKEN_INDEX, END_TOKEN_INDEX)\n",
        "    )\n",
        "    df['Report_Seq'] = df['Report_Seq'].apply(lambda x: np.array(x, dtype=np.int64))\n",
        "\n",
        "\n",
        "class ReportDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        # Filter rows where either image feature is missing\n",
        "        valid_rows = dataframe[\n",
        "            dataframe['image_feat_img1'].notna() &\n",
        "            dataframe['image_feat_img2'].notna()\n",
        "        ].copy() # Add .copy() to avoid SettingWithCopyWarning later\n",
        "\n",
        "        # --- DIAGNOSTIC PRINT HERE ---\n",
        "        sample_row = valid_rows.iloc[0] if not valid_rows.empty else None\n",
        "        if sample_row is not None:\n",
        "            if isinstance(sample_row['image_feat_img1'], np.ndarray) and isinstance(sample_row['image_feat_img2'], np.ndarray):\n",
        "                print(f\"Diagnostic: First valid row - Shape of img1: {sample_row['image_feat_img1'].shape}\")\n",
        "                print(f\"Diagnostic: First valid row - Shape of img2: {sample_row['image_feat_img2'].shape}\")\n",
        "                concatenated_shape = np.concatenate((sample_row['image_feat_img1'], sample_row['image_feat_img2'])).shape\n",
        "                print(f\"Diagnostic: First valid row - Shape after concat: {concatenated_shape}\")\n",
        "            else:\n",
        "                print(\"Diagnostic: Image features are not numpy arrays, check feature extraction step.\")\n",
        "                print(f\"Type of image_feat_img1: {type(sample_row['image_feat_img1'])}\")\n",
        "                print(f\"Value of image_feat_img1: {sample_row['image_feat_img1']}\")\n",
        "                print(f\"Type of image_feat_img2: {type(sample_row['image_feat_img2'])}\")\n",
        "                print(f\"Value of image_feat_img2: {sample_row['image_feat_img2']}\")\n",
        "        else:\n",
        "            print(\"Diagnostic: No valid rows found in dataframe for dataset creation.\")\n",
        "        # --- END DIAGNOSTIC ---\n",
        "\n",
        "        # Concatenate features and ensure float32\n",
        "        self.image_features = np.array([\n",
        "            np.concatenate((row['image_feat_img1'], row['image_feat_img2']))\n",
        "            for index, row in valid_rows.iterrows()\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        self.report_sequences = np.array([\n",
        "            row['Report_Seq']\n",
        "            for index, row in valid_rows.iterrows()\n",
        "        ], dtype=np.int64)\n",
        "\n",
        "        # Corrected calculation for report_lengths_for_packing:\n",
        "        # This length should reflect the number of non-padded tokens in the *input* to the decoder.\n",
        "        # The input to the decoder is `captions[:, :-1]`, so we need to account for that.\n",
        "        self.report_lengths_for_packing = []\n",
        "        for seq in self.report_sequences:\n",
        "            # Find the actual length of the sequence before padding or truncation by MAX_SEQ_LEN\n",
        "            # This counts tokens until PAD_TOKEN or end of MAX_SEQ_LEN\n",
        "            true_len_before_padding = 0\n",
        "            for token_id in seq:\n",
        "                if token_id == PAD_TOKEN_INDEX:\n",
        "                    break\n",
        "                true_len_before_padding += 1\n",
        "\n",
        "            # The input to the decoder (`captions_input = captions[:, :-1]`)\n",
        "            # excludes the <end> token (if present) and the padding at the very end.\n",
        "            # It also implicitly excludes the <start> token if we consider it already consumed.\n",
        "            # The length needed for loss masking is the count of *target* tokens.\n",
        "            # Targets are `captions[:, 1:]`.\n",
        "            # If the original sequence length is L (including START and END),\n",
        "            # then `captions[:, 1:]` has length L-1 (excluding START).\n",
        "            # We want the number of *non-padding* tokens in `captions[:, 1:]`.\n",
        "\n",
        "            # Count non-pad tokens in the original (full) `Report_Seq`\n",
        "            actual_non_pad_tokens = (seq != PAD_TOKEN_INDEX).sum()\n",
        "\n",
        "            # The length of the `targets` tensor is `actual_non_pad_tokens - 1`\n",
        "            # (since `<start>` is removed and not a target).\n",
        "            # Ensure it's at least 1 (for an <end> token if nothing else).\n",
        "            length_for_loss = max(1, actual_non_pad_tokens - 1)\n",
        "\n",
        "            self.report_lengths_for_packing.append(length_for_loss)\n",
        "\n",
        "        self.report_lengths_for_packing = np.array(self.report_lengths_for_packing, dtype=np.int64)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_feat = torch.tensor(self.image_features[idx], dtype=torch.float32)\n",
        "        report_seq = torch.tensor(self.report_sequences[idx], dtype=torch.long)\n",
        "        # Return the **corrected** length for packing\n",
        "        report_len_for_packing = torch.tensor(self.report_lengths_for_packing[idx], dtype=torch.long)\n",
        "\n",
        "        return image_feat, report_seq, report_len_for_packing\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import collections\n",
        "import itertools\n",
        "import nltk\n",
        "\n",
        "train_dataset = ReportDataset(train_df)\n",
        "val_dataset = ReportDataset(val_df)\n",
        "test_dataset = ReportDataset(test_df)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True if DEVICE=='cuda' else False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True if DEVICE=='cuda' else False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True if DEVICE=='cuda' else False)\n",
        "\n",
        "print(\"\\nText preprocessing and dataset creation complete. DataLoaders ready.\")\n",
        "\n",
        "try:\n",
        "    first_train_batch = next(iter(train_dataloader))\n",
        "    image_features_batch = first_train_batch[0]\n",
        "    captions_batch = first_train_batch[1]\n",
        "    lengths_batch = first_train_batch[2]\n",
        "\n",
        "    print(f\"First training batch Image features shape: {image_features_batch.shape}\")\n",
        "    print(f\"First training batch Report sequences shape: {captions_batch.shape}\")\n",
        "    print(f\"First training batch Report lengths shape: {lengths_batch.shape}\")\n",
        "\n",
        "    if 'encoder' in globals() and hasattr(encoder, 'encoder_linear'):\n",
        "        print(f\"EncoderCNN expected input dimension (from encoder.encoder_linear.in_features): {encoder.encoder_linear.in_features}\")\n",
        "    else:\n",
        "        print(\"Encoder model not yet instantiated, cannot check its expected input dimension.\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"Cannot perform full diagnostic: Encoder or other global variables not yet defined. Please run previous cells first.\")\n",
        "\n",
        "print(\"\\n--- Diagnostic: DataFrame Column Check ---\")\n",
        "print(f\"Columns in train_df: {train_df.columns.tolist()}\")\n",
        "print(f\"Columns in val_df: {val_df.columns.tolist()}\")\n",
        "print(f\"Columns in test_df: {test_df.columns.tolist()}\")\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "print(\"\\n--- Diagnostic: Head of test_df ---\")\n",
        "print(test_df.head())\n",
        "print(\"----------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GnQsIzFwhH7V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnQsIzFwhH7V",
        "outputId": "013d93b5-389f-4834-8ef2-d71221e42311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model architectures defined and initialized with Attention Mechanism.\n",
            "Encoder: EncoderCNN(\n",
            "  (encoder_linear): Linear(in_features=1024, out_features=768, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (batch_norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Decoder: DecoderRNN(\n",
            "  (embed): Embedding(822, 768)\n",
            "  (attention): Attention(\n",
            "    (encoder_att): Linear(in_features=768, out_features=1536, bias=True)\n",
            "    (decoder_att): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "    (full_att): Linear(in_features=1536, out_features=1, bias=True)\n",
            "    (relu): ReLU()\n",
            "    (softmax): Softmax(dim=1)\n",
            "  )\n",
            "  (lstm): LSTM(1536, 1536, num_layers=3, batch_first=True, dropout=0.3)\n",
            "  (linear): Linear(in_features=1536, out_features=822, bias=True)\n",
            "  (feature_to_hidden): Linear(in_features=768, out_features=1536, bias=True)\n",
            "  (feature_to_cell): Linear(in_features=768, out_features=1536, bias=True)\n",
            "  (dropout_embed): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Cell-3\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F # Import F for functional operations like softmax\n",
        "\n",
        "# --- EncoderCNN Model ---\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, original_feature_dim): # CORRECTED: __init__\n",
        "        \"\"\"\n",
        "        Initializes the EncoderCNN.\n",
        "        Args:\n",
        "            embed_size (int): The desired output dimension of the image features,\n",
        "                              which will be the input dimension for the decoder's initial state\n",
        "                              and also used as the 'encoder_dim' for the Attention module.\n",
        "            original_feature_dim (int): The dimension of the raw image features from the pre-trained model.\n",
        "                                        This should be 2048 if concatenating two 1024-dim features.\n",
        "        \"\"\"\n",
        "        super(EncoderCNN, self).__init__() # CORRECTED: __init__\n",
        "        # Projects the original image features to the desired embedding size.\n",
        "        self.encoder_linear = nn.Linear(original_feature_dim, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        # It's good practice to use a global DROPOUT variable if defined in Cell 1\n",
        "        # For now, keeping 0.5 as in your original code.\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.batch_norm = nn.BatchNorm1d(embed_size) # Added Batch Normalization\n",
        "\n",
        "    def forward(self, image_features):\n",
        "        \"\"\"\n",
        "        Forward pass for the EncoderCNN.\n",
        "        Args:\n",
        "            image_features (torch.Tensor): Tensor of shape (batch_size, original_feature_dim)\n",
        "                                            containing the extracted image features.\n",
        "        Returns:\n",
        "            torch.Tensor: Encoded image features of shape (batch_size, embed_size).\n",
        "                          This output will serve as the context for the attention mechanism.\n",
        "        \"\"\"\n",
        "        # Apply linear projection, Batch Norm, ReLU, and Dropout\n",
        "        features = self.encoder_linear(image_features)\n",
        "        features = self.batch_norm(features)\n",
        "        features = self.relu(features)\n",
        "        features = self.dropout(features)\n",
        "        return features\n",
        "\n",
        "\n",
        "# --- Attention Mechanism ---\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_hidden_dim, attention_dim): # CORRECTED: __init__\n",
        "        \"\"\"\n",
        "        Initializes the Attention module. This module computes attention weights\n",
        "        over the encoder's output features based on the decoder's current hidden state.\n",
        "\n",
        "        Args:\n",
        "            encoder_dim (int): Feature size of the encoded image (output of EncoderCNN, i.e., EMBED_SIZE).\n",
        "            decoder_hidden_dim (int): Size of the decoder's LSTM hidden state (HIDDEN_SIZE).\n",
        "            attention_dim (int): Dimension of the attention linear layers (e.g., HIDDEN_SIZE or EMBED_SIZE).\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__() # CORRECTED: __init__\n",
        "        # Linear layer to transform encoder's output for attention\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
        "        # Linear layer to transform decoder's hidden state for attention\n",
        "        self.decoder_att = nn.Linear(decoder_hidden_dim, attention_dim)\n",
        "        # Linear layer to calculate attention \"energies\" (scores)\n",
        "        self.full_att = nn.Linear(attention_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # Softmax layer to calculate attention distribution\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward pass for the Attention module.\n",
        "\n",
        "        Args:\n",
        "            encoder_out (torch.Tensor): Encoded image features from EncoderCNN,\n",
        "                                        shape (batch_size, encoder_dim).\n",
        "            decoder_hidden (torch.Tensor): Previous decoder's last-layer hidden state,\n",
        "                                           shape (batch_size, decoder_hidden_dim).\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - torch.Tensor: Attention-weighted context vector, shape (batch_size, encoder_dim).\n",
        "                - torch.Tensor: Attention weights, shape (batch_size, 1).\n",
        "        \"\"\"\n",
        "        # (batch_size, attention_dim)\n",
        "        att1 = self.encoder_att(encoder_out)\n",
        "        # (batch_size, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden)\n",
        "\n",
        "        # Sum the transformed encoder output and decoder hidden state, apply ReLU, then pass through full_att\n",
        "        # The result is (batch_size, 1), representing an \"energy\" score for the global image feature\n",
        "        energy = self.full_att(self.relu(att1 + att2))\n",
        "\n",
        "        # Apply softmax to get attention weights (sum to 1)\n",
        "        alpha = self.softmax(energy) # (batch_size, 1)\n",
        "\n",
        "        # Compute the context vector by multiplying weights with encoder_out\n",
        "        # (batch_size, 1) * (batch_size, encoder_dim) -> (batch_size, encoder_dim)\n",
        "        context = alpha * encoder_out\n",
        "\n",
        "        return context, alpha\n",
        "\n",
        "\n",
        "# --- DecoderRNN Model with Attention ---\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout_rate, encoder_output_dim, attention_dim): # CORRECTED: __init__\n",
        "        \"\"\"\n",
        "        Initializes the DecoderRNN with an Attention mechanism.\n",
        "\n",
        "        Args:\n",
        "            embed_size (int): Dimension of word embeddings.\n",
        "            hidden_size (int): The number of features in the hidden state of the LSTM.\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "            num_layers (int): Number of recurrent layers.\n",
        "            dropout_rate (float): Dropout probability.\n",
        "            encoder_output_dim (int): The dimension of the features coming out of the EncoderCNN.\n",
        "                                      This is used for initializing LSTM states AND as encoder_dim for Attention.\n",
        "            attention_dim (int): Dimension of the attention linear layers.\n",
        "        \"\"\"\n",
        "        super(DecoderRNN, self).__init__() # CORRECTED: __init__\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Initialize the Attention module\n",
        "        self.attention = Attention(encoder_output_dim, hidden_size, attention_dim)\n",
        "\n",
        "        # LSTM layer: input size is embed_size (word embedding) + encoder_output_dim (context vector from attention)\n",
        "        self.lstm = nn.LSTM(embed_size + encoder_output_dim, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
        "\n",
        "        # Linear layer to project LSTM output to vocabulary size\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Linear layers to project the encoder's output features to the initial\n",
        "        # hidden and cell states of the LSTM.\n",
        "        self.feature_to_hidden = nn.Linear(encoder_output_dim, hidden_size)\n",
        "        self.feature_to_cell = nn.Linear(encoder_output_dim, hidden_size)\n",
        "\n",
        "        self.dropout_embed = nn.Dropout(dropout_rate) # Dropout applied to word embeddings\n",
        "\n",
        "\n",
        "    def forward(self, features, captions_input, lengths):\n",
        "        \"\"\"\n",
        "        Forward pass for the DecoderRNN with Attention.\n",
        "\n",
        "        Args:\n",
        "            features (torch.Tensor): Encoded image features from the EncoderCNN,\n",
        "                                     shape (batch_size, encoder_output_dim).\n",
        "            captions_input (torch.Tensor): Tokenized caption sequences (shifted, includes <start>),\n",
        "                                            shape (batch_size, max_seq_len-1).\n",
        "                                            Contains token IDs.\n",
        "            lengths (torch.Tensor): Actual lengths of each sequence in the batch (before padding).\n",
        "                                    Shape (batch_size,).\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted logits for each token at each timestep,\n",
        "                          shape (batch_size, max_seq_len-1, vocab_size).\n",
        "        \"\"\"\n",
        "        batch_size = features.size(0)\n",
        "        max_seq_len_minus_one = captions_input.size(1) # This is the length of the input captions after <start>\n",
        "\n",
        "        # Initialize LSTM hidden and cell states using the encoded image features\n",
        "        # States need to be (num_layers, batch_size, hidden_size)\n",
        "        h_t = self.feature_to_hidden(features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
        "        c_t = self.feature_to_cell(features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
        "\n",
        "        # Get word embeddings for the input captions\n",
        "        embeddings = self.dropout_embed(self.embed(captions_input)) # (batch_size, max_seq_len-1, embed_size)\n",
        "\n",
        "        # Tensor to store decoder outputs (logits)\n",
        "        # We need vocab_size here because these are the final predictions\n",
        "        outputs = torch.zeros(batch_size, max_seq_len_minus_one, self.linear.out_features).to(features.device)\n",
        "\n",
        "        # Loop through the sequence, one word at a time, to apply attention dynamically\n",
        "        for t in range(max_seq_len_minus_one):\n",
        "            # Get the hidden state of the LAST LSTM layer for attention calculation\n",
        "            # h_t is (num_layers, batch_size, hidden_size). We need the last layer for attention.\n",
        "            current_decoder_hidden_for_att = h_t[-1, :, :] # (batch_size, hidden_size)\n",
        "\n",
        "            # Calculate context vector using attention\n",
        "            context_vector, _ = self.attention(features, current_decoder_hidden_for_att) # (batch_size, encoder_output_dim)\n",
        "\n",
        "            # Get the current word's embedding: embeddings[:, t, :] is (batch_size, embed_size)\n",
        "            current_word_embedding = embeddings[:, t, :]\n",
        "\n",
        "            # Concatenate embedded word with context vector for LSTM input\n",
        "            # Result: (batch_size, embed_size + encoder_output_dim)\n",
        "            combined_input = torch.cat((current_word_embedding, context_vector), dim=1).unsqueeze(1) # (batch_size, 1, embed_size + encoder_output_dim)\n",
        "\n",
        "            # Pass combined input through LSTM, updating hidden and cell states\n",
        "            # output is (batch_size, 1, hidden_size)\n",
        "            # h_t, c_t are (num_layers, batch_size, hidden_size)\n",
        "            output, (h_t, c_t) = self.lstm(combined_input, (h_t, c_t))\n",
        "\n",
        "            # Project LSTM output to vocabulary size to get logits\n",
        "            # output.squeeze(1) converts (batch_size, 1, hidden_size) to (batch_size, hidden_size)\n",
        "            prediction_logits = self.linear(output.squeeze(1)) # (batch_size, vocab_size)\n",
        "\n",
        "            # Store the logits for this timestep\n",
        "            outputs[:, t, :] = prediction_logits\n",
        "\n",
        "        # Return the raw logits for subsequent loss calculation\n",
        "        return outputs\n",
        "\n",
        "# --- Model Instantiation ---\n",
        "# IMPORTANT: Ensure ORIGINAL_FEATURE_DIM in Cell 1 is set to 2048 if you concatenated two 1024-dim features.\n",
        "# Make sure EMBED_SIZE, HIDDEN_SIZE, VOCAB_SIZE, NUM_LAYERS, DROPOUT, DEVICE are defined in Cell 1.\n",
        "\n",
        "# Define ATTENTION_DIM - this is a new hyperparameter.\n",
        "# A common choice is to set it equal to HIDDEN_SIZE or EMBED_SIZE, or some intermediate value.\n",
        "# Let's use HIDDEN_SIZE for a start.\n",
        "ATTENTION_DIM = HIDDEN_SIZE\n",
        "\n",
        "encoder = EncoderCNN(EMBED_SIZE, ORIGINAL_FEATURE_DIM).to(DEVICE)\n",
        "\n",
        "# Pass the new 'attention_dim' to the DecoderRNN constructor\n",
        "decoder = DecoderRNN(EMBED_SIZE, HIDDEN_SIZE, VOCAB_SIZE, NUM_LAYERS, DROPOUT, EMBED_SIZE, ATTENTION_DIM).to(DEVICE)\n",
        "\n",
        "print(\"Model architectures defined and initialized with Attention Mechanism.\")\n",
        "print(f\"Encoder: {encoder}\")\n",
        "print(f\"Decoder: {decoder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SHHi43i8YwFg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SHHi43i8YwFg",
        "outputId": "57d31538-d402-4abc-b969-52230face7bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Training Loss: 4.4629\n",
            "Epoch 1 Validation Loss: 3.8676\n",
            "Validation loss improved from inf to 3.8676. Saving models...\n",
            "Epoch 2 Training Loss: 3.3730\n",
            "Epoch 2 Validation Loss: 2.7522\n",
            "Validation loss improved from 3.8676 to 2.7522. Saving models...\n",
            "Epoch 3 Training Loss: 2.6152\n",
            "Epoch 3 Validation Loss: 2.2939\n",
            "Validation loss improved from 2.7522 to 2.2939. Saving models...\n",
            "Epoch 4 Training Loss: 2.2789\n",
            "Epoch 4 Validation Loss: 2.0510\n",
            "Validation loss improved from 2.2939 to 2.0510. Saving models...\n",
            "Epoch 5 Training Loss: 2.0721\n",
            "Epoch 5 Validation Loss: 1.9005\n",
            "Validation loss improved from 2.0510 to 1.9005. Saving models...\n",
            "Epoch 6 Training Loss: 1.9272\n",
            "Epoch 6 Validation Loss: 1.8098\n",
            "Validation loss improved from 1.9005 to 1.8098. Saving models...\n",
            "Epoch 7 Training Loss: 1.8158\n",
            "Epoch 7 Validation Loss: 1.7181\n",
            "Validation loss improved from 1.8098 to 1.7181. Saving models...\n",
            "Epoch 8 Training Loss: 1.7191\n",
            "Epoch 8 Validation Loss: 1.6477\n",
            "Validation loss improved from 1.7181 to 1.6477. Saving models...\n",
            "Epoch 9 Training Loss: 1.6334\n",
            "Epoch 9 Validation Loss: 1.5959\n",
            "Validation loss improved from 1.6477 to 1.5959. Saving models...\n",
            "Epoch 10 Training Loss: 1.5572\n",
            "Epoch 10 Validation Loss: 1.5505\n",
            "Validation loss improved from 1.5959 to 1.5505. Saving models...\n",
            "Epoch 11 Training Loss: 1.4892\n",
            "Epoch 11 Validation Loss: 1.5225\n",
            "Validation loss improved from 1.5505 to 1.5225. Saving models...\n",
            "Epoch 12 Training Loss: 1.4293\n",
            "Epoch 12 Validation Loss: 1.4896\n",
            "Validation loss improved from 1.5225 to 1.4896. Saving models...\n",
            "Epoch 13 Training Loss: 1.3694\n",
            "Epoch 13 Validation Loss: 1.4567\n",
            "Validation loss improved from 1.4896 to 1.4567. Saving models...\n",
            "Epoch 14 Training Loss: 1.3159\n",
            "Epoch 14 Validation Loss: 1.4356\n",
            "Validation loss improved from 1.4567 to 1.4356. Saving models...\n",
            "Epoch 15 Training Loss: 1.2618\n",
            "Epoch 15 Validation Loss: 1.4131\n",
            "Validation loss improved from 1.4356 to 1.4131. Saving models...\n",
            "Epoch 16 Training Loss: 1.2137\n",
            "Epoch 16 Validation Loss: 1.4026\n",
            "Validation loss improved from 1.4131 to 1.4026. Saving models...\n",
            "Epoch 17 Training Loss: 1.1613\n",
            "Epoch 17 Validation Loss: 1.3889\n",
            "Validation loss improved from 1.4026 to 1.3889. Saving models...\n",
            "Epoch 18 Training Loss: 1.1114\n",
            "Epoch 18 Validation Loss: 1.3786\n",
            "Validation loss improved from 1.3889 to 1.3786. Saving models...\n",
            "Epoch 19 Training Loss: 1.0702\n",
            "Epoch 19 Validation Loss: 1.3713\n",
            "Validation loss improved from 1.3786 to 1.3713. Saving models...\n",
            "Epoch 20 Training Loss: 1.0206\n",
            "Epoch 20 Validation Loss: 1.3719\n",
            "Validation loss did not improve. Best was 1.3713.\n",
            "Epoch 21 Training Loss: 0.9775\n",
            "Epoch 21 Validation Loss: 1.3686\n",
            "Validation loss improved from 1.3713 to 1.3686. Saving models...\n",
            "Epoch 22 Training Loss: 0.9409\n",
            "Epoch 22 Validation Loss: 1.3748\n",
            "Validation loss did not improve. Best was 1.3686.\n",
            "Epoch 23 Training Loss: 0.8990\n",
            "Epoch 23 Validation Loss: 1.3733\n",
            "Validation loss did not improve. Best was 1.3686.\n",
            "Epoch 24 Training Loss: 0.8648\n",
            "Epoch 24 Validation Loss: 1.3880\n",
            "Validation loss did not improve. Best was 1.3686.\n",
            "Epoch 25 Training Loss: 0.8184\n",
            "Epoch 25 Validation Loss: 1.3831\n",
            "Validation loss did not improve. Best was 1.3686.\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Cell 4: Training Loop\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # For learning rate scheduling\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"Starting training process...\")\n",
        "\n",
        "# --- Hyperparameters and Global Variables (ensure these are defined in Cell 1) ---\n",
        "# Assuming these are defined:\n",
        "# DEVICE, NUM_EPOCHS, LEARNING_RATE, GRADIENT_CLIP, LOG_INTERVAL\n",
        "# MODEL_SAVE_DIR, MODEL_SAVE_PATH_ENCODER, MODEL_SAVE_PATH_DECODER\n",
        "# vocab (from your Vocabulary class), PAD_TOKEN_INDEX\n",
        "# encoder, decoder (from Cell 3 instantiation)\n",
        "# train_dataloader, val_dataloader (from Cell 2 data loading)\n",
        "\n",
        "# Ensure models are initialized and moved to the correct device\n",
        "encoder.to(DEVICE)\n",
        "decoder.to(DEVICE)\n",
        "\n",
        "# --- Loss Function and Optimizer ---\n",
        "# Using CrossEntropyLoss and ignoring predictions for padding tokens\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_INDEX)\n",
        "\n",
        "# Combined parameters for optimization\n",
        "params = list(encoder.encoder_linear.parameters()) + \\\n",
        "         list(encoder.batch_norm.parameters()) + \\\n",
        "         list(decoder.embed.parameters()) + \\\n",
        "         list(decoder.lstm.parameters()) + \\\n",
        "         list(decoder.linear.parameters()) + \\\n",
        "         list(decoder.feature_to_hidden.parameters()) + \\\n",
        "         list(decoder.feature_to_cell.parameters()) + \\\n",
        "         list(decoder.attention.parameters()) # Include attention parameters\n",
        "\n",
        "optimizer = optim.Adam(params, lr=LEARNING_RATE)\n",
        "\n",
        "# Learning Rate Scheduler: Reduces LR when a metric (e.g., validation loss) has stopped improving.\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_model(encoder, decoder, criterion, optimizer, train_loader, val_loader,\n",
        "                num_epochs, device, grad_clip, log_interval, scheduler):\n",
        "    best_val_loss = float('inf')\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        # Training Phase\n",
        "        encoder.train() # Set models to training mode\n",
        "        decoder.train()\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_idx, (image_feats, captions, lengths) in enumerate(train_loader):\n",
        "            # Move data to device\n",
        "            image_feats = image_feats.to(device)\n",
        "            # Captions contain <start> at the beginning, we want to predict words after <start>\n",
        "            # So, input to decoder is captions[:, :-1]\n",
        "            # Target for loss is captions[:, 1:]\n",
        "            captions_input = captions[:, :-1].to(device) # Input sequence (shifted right by one)\n",
        "            targets = captions[:, 1:].to(device)         # Target sequence (original, without <start>)\n",
        "\n",
        "            # Original lengths refer to full caption length including <start> and <end>\n",
        "            # For the decoder input (captions_input), lengths should be decremented by 1.\n",
        "            # This is important for handling padding correctly in the loss calculation\n",
        "            # even though the LSTM loop is word-by-word.\n",
        "            # We use lengths for masking the loss, not for pack_padded_sequence.\n",
        "            actual_lengths_for_targets = [l - 1 for l in lengths] # length of captions_input/targets\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            encoded_features = encoder(image_feats)\n",
        "            # Pass image features, captions_input, and actual_lengths_for_targets to decoder\n",
        "            # The decoder now handles sequence processing internally with attention\n",
        "            outputs = decoder(encoded_features, captions_input, torch.tensor(actual_lengths_for_targets).to(device))\n",
        "\n",
        "            # Reshape outputs and targets for CrossEntropyLoss\n",
        "            # outputs: (batch_size, max_seq_len-1, vocab_size) -> (batch_size * (max_seq_len-1), vocab_size)\n",
        "            # targets: (batch_size, max_seq_len-1) -> (batch_size * (max_seq_len-1))\n",
        "            outputs_flat = outputs.contiguous().view(-1, outputs.shape[-1])\n",
        "            targets_flat = targets.contiguous().view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs_flat, targets_flat)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient Clipping: Prevents exploding gradients, common in RNNs\n",
        "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
        "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % log_interval == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], \"\n",
        "                      f\"Loss: {loss.item():.4f}, Time/Batch: {elapsed / log_interval:.4f}s\")\n",
        "                start_time = time.time() # Reset timer\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        training_losses.append(avg_train_loss)\n",
        "        print(f\"Epoch {epoch} Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation Phase\n",
        "        encoder.eval() # Set models to evaluation mode\n",
        "        decoder.eval()\n",
        "        val_total_loss = 0\n",
        "        with torch.no_grad(): # Disable gradient calculations for validation\n",
        "            for batch_idx_val, (image_feats_val, captions_val, lengths_val) in enumerate(val_loader):\n",
        "                image_feats_val = image_feats_val.to(device)\n",
        "                captions_input_val = captions_val[:, :-1].to(device)\n",
        "                targets_val = captions_val[:, 1:].to(device)\n",
        "                actual_lengths_for_targets_val = [l - 1 for l in lengths_val]\n",
        "\n",
        "                encoded_features_val = encoder(image_feats_val)\n",
        "                outputs_val = decoder(encoded_features_val, captions_input_val, torch.tensor(actual_lengths_for_targets_val).to(device))\n",
        "\n",
        "                outputs_flat_val = outputs_val.contiguous().view(-1, outputs_val.shape[-1])\n",
        "                targets_flat_val = targets_val.contiguous().view(-1)\n",
        "\n",
        "                val_loss = criterion(outputs_flat_val, targets_flat_val)\n",
        "                val_total_loss += val_loss.item()\n",
        "\n",
        "        avg_val_loss = val_total_loss / len(val_loader)\n",
        "        validation_losses.append(avg_val_loss)\n",
        "        print(f\"Epoch {epoch} Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning Rate Scheduler Step\n",
        "        scheduler.step(avg_val_loss) # Pass validation loss to scheduler\n",
        "\n",
        "        # Save model if validation loss improves\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving models...\")\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(encoder.state_dict(), MODEL_SAVE_PATH_ENCODER)\n",
        "            torch.save(decoder.state_dict(), MODEL_SAVE_PATH_DECODER)\n",
        "        else:\n",
        "            print(f\"Validation loss did not improve. Best was {best_val_loss:.4f}.\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return training_losses, validation_losses\n",
        "\n",
        "\n",
        "# --- Run Training ---\n",
        "if __name__ == '__main__':\n",
        "    # Ensure all necessary global variables are defined before this cell runs\n",
        "    # (e.g., EMBED_SIZE, HIDDEN_SIZE, VOCAB_SIZE, NUM_LAYERS, DROPOUT, ORIGINAL_FEATURE_DIM,\n",
        "    # DEVICE, NUM_EPOCHS, LEARNING_RATE, GRADIENT_CLIP, LOG_INTERVAL,\n",
        "    # MODEL_SAVE_DIR, MODEL_SAVE_PATH_ENCODER, MODEL_SAVE_PATH_DECODER,\n",
        "    # encoder, decoder, train_dataloader, val_dataloader, PAD_TOKEN_INDEX)\n",
        "\n",
        "    # Example placeholders if not defined globally in previous cells\n",
        "    # from your_data_utils import PAD_TOKEN_INDEX, START_TOKEN, END_TOKEN, UNK_TOKEN, vocab, idx_to_word\n",
        "    # from your_model_defs import EncoderCNN, DecoderRNN # Make sure these are the updated ones\n",
        "\n",
        "    # This block assumes your encoder and decoder were initialized in Cell 3\n",
        "    # and moved to DEVICE.\n",
        "\n",
        "    training_losses, validation_losses = train_model(\n",
        "        encoder, decoder, criterion, optimizer, train_dataloader, val_dataloader,\n",
        "        NUM_EPOCHS, DEVICE, GRADIENT_CLIP, LOG_INTERVAL, scheduler\n",
        "    )\n",
        "\n",
        "    # --- Plotting Loss (Optional, for visualization) ---\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_losses, label='Training Loss')\n",
        "    plt.plot(validation_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KHJMHbBrwVGs",
      "metadata": {
        "id": "KHJMHbBrwVGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764e6cd2-5985-44ec-8226-634bcdd78154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data for evaluation metrics...\n",
            "NLTK data download complete.\n",
            "Starting model evaluation and inference...\n",
            "Models (encoder and decoder) already found in memory from training.\n",
            "\n",
            "Generating captions for test set and collecting metrics...\n",
            "\n",
            "Calculating BLEU scores...\n",
            "Average BLEU-1 Score: 0.3222\n",
            "Average BLEU-2 Score: 0.2007\n",
            "Average BLEU-3 Score: 0.1231\n",
            "Average BLEU-4 Score: 0.0865\n",
            "\n",
            "Calculating ROUGE scores...\n",
            "Average ROUGE-1 F-measure: 0.3924\n",
            "Average ROUGE-2 F-measure: 0.1282\n",
            "Average ROUGE-L F-measure: 0.3261\n",
            "\n",
            "Calculating METEOR score...\n",
            "Average METEOR Score: 0.3098\n",
            "\n",
            "Calculating CIDEr score :\n",
            "CIDEr Score: 0.4304\n",
            "\n",
            "--- Sample Generated Captions ---\n",
            "\n",
            "--- Sample 1 ---\n",
            "Image ID in DataFrame: NLMCXR_png/CXR154_IM-0350_0\n",
            "Original Report: startseq cardiomediastinal silhouette normal . pulmonary vasculature and are normal . no consolidation pneumothora large pleural effusion . osseous structures and soft tissues are normal . endseq\n",
            "Generated Caption: startseq heart size normal . lungs are clear . are normal . no pneumonia effusions edema pneumothora adenopathy nodules masses . endseq\n",
            "\n",
            "--- Sample 2 ---\n",
            "Image ID in DataFrame: NLMCXR_png/CXR1531_IM-0344_0\n",
            "Original Report: startseq heart size within normal limits stable mediastinal contours with aortic ectasiatortuosity . left hilar and left lower lobe calcifications indicate previous granulomatous process . no alveolar consolidation no findings pleural effusion pulmonary edema . no pneumothora . endseq\n",
            "Generated Caption: startseq heart size normal . lungs are clear . are normal . no pneumonia effusions edema pneumothora adenopathy nodules masses . endseq\n",
            "\n",
            "--- Sample 3 ---\n",
            "Image ID in DataFrame: NLMCXR_png/CXR1557_IM-0364\n",
            "Original Report: startseq there interval removal the tracheostomy tube and right subclavian central venous catheter . the cardiac silhouette mildly enlarged . there are pulmonary opacities with blunting the bilateral costophrenic the right greater than left with pleural thickening versus loculated pleural fluid along the peripheral aspect the right upper lobe . right perihilar opacities . no pneumothora identified . endseq\n",
            "Generated Caption: startseq heart size within normal limits . no focal airspace disease . no pneumothora effusions . endseq\n",
            "\n",
            "--- Sample 4 ---\n",
            "Image ID in DataFrame: NLMCXR_png/CXR1532_IM-0344_0\n",
            "Original Report: startseq both lungs are clear and epanded . heart and mediastinum normal . endseq\n",
            "Generated Caption: startseq the lungs are clear . the cardiomediastinal silhouette within normal limits . no pneumothora pleural effusion . endseq\n",
            "\n",
            "--- Sample 5 ---\n",
            "Image ID in DataFrame: NLMCXR_png/CXR1515_IM-0333_0\n",
            "Original Report: startseq the cardiomediastinal silhouette within normal limits for appearance . no focal areas pulmonary consolidation . no pneumothora . no pleural effusion . the thoracic spine appears intact . endseq\n",
            "Generated Caption: startseq the heart size normal . the mediastinal contour within normal limits . the lungs are free any focal infiltrates . there are no nodules masses . no visible pneumothora . no visible pleural fluid . the are grossly normal . there no visible free intraperitoneal air under the diaphragm . endseq\n",
            "\n",
            "--- Sample 6 ---\n",
            "Image ID in DataFrame: NLMCXR_png/CXR1540_IM-0351_0\n",
            "Original Report: startseq cardiomediastinal silhouette and pulmonary vasculature are within normal limits . lungs are clear . no pneumothora pleural effusion . no acute osseous findings . endseq\n",
            "Generated Caption: startseq the heart normal size . the mediastinum unremarkable . the lungs are clear . endseq\n",
            "\n",
            "--- Sample 7 ---\n",
            "Image ID in DataFrame: NLMCXR_png/CXR1524_IM-0339_0\n",
            "Original Report: startseq heart size normal . the aorta tortuous and cannot eclude ascending aortic aneurysm . the pulmonary vascularity normal . there residual prior granulomatous infection . lungs are otherwise clear . degenerative change the spine . endseq\n",
            "Generated Caption: startseq heart size within normal limits . no focal airspace disease . no pneumothora effusions . endseq\n",
            "\n",
            "--- Sample 8 ---\n",
            "Image ID in DataFrame: NLMCXR_png/CXR1521_IM-0337_2\n",
            "Original Report: startseq three images are available for review . the heart size normal . the mediastinal contour within normal limits . the lungs are free any focal infiltrates . there are no nodules masses . no visible pneumothora . no visible pleural fluid . the are grossly normal . there no visible free intraperitoneal air under the diaphragm . endseq\n",
            "Generated Caption: startseq the heart and lungs have the interval . both lungs are clear and epanded . heart and mediastinum normal . endseq\n",
            "\n",
            "--- Sample 9 ---\n",
            "Image ID in DataFrame: NLMCXR_png/CXR1518_IM-0335_0\n",
            "Original Report: startseq no focal consolidation pneumothora pleural effusion . cardiomediastinal silhouette stable and unremarkable . no acute osseous abnormalities are identified . endseq\n",
            "Generated Caption: startseq the lungs are clear . there no pleural effusion pneumothora . the heart and mediastinum are normal . the skeletal structures are normal . endseq\n",
            "\n",
            "--- Sample 10 ---\n",
            "Image ID in DataFrame: NLMCXR_png/CXR1525_IM-0340_3\n",
            "Original Report: startseq images . there large hydropneumothora within the left chest . there essentially complete collapse the left lung . within the right lung there are increased interstitial opacities within the medial right lung base and right upper lobe with patchy airspace opacity within the right lung ape . the right lung ape there more focal ovoid lucency which measures approimately cm . this could indicate cavitation . leftsided cardiomediastinal contours are obscured collapse the left lung . no convincing acute bony findings . endseq\n",
            "Generated Caption: startseq the cardiomediastinal silhouette and vasculature are within normal limits for size and contour . the lungs are normally inflated and clear . osseous structures are within normal limits for patient age . endseq\n",
            "\n",
            "Model evaluation and inference complete.\n"
          ]
        }
      ],
      "source": [
        "# Cell 5- Model Evaluation and Inference\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F # Import F for log_softmax\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import random\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import matplotlib.pyplot as plt # Import for plotting\n",
        "from PIL import Image # Import for image loading\n",
        "\n",
        "# Import COCO evaluation tools (should be installed via pip install pycocotools)\n",
        "# Make sure pycocotools and pycocoevalcap are installed:\n",
        "# !pip install pycocotools\n",
        "# !pip install pycocoevalcap\n",
        "from pycocotools.coco import COCO\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "# The PTBTokenizer is usually found within pycocoevalcap\n",
        "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
        "\n",
        "\n",
        "# Make sure NLTK data is available for tokenization and METEOR\n",
        "# These should ideally be in Cell 1, but added here as a safeguard for evaluation\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/wordnet') # For METEOR\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK data for evaluation metrics...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    print(\"NLTK data download complete.\")\n",
        "\n",
        "print(\"Starting model evaluation and inference...\")\n",
        "\n",
        "# --- 1. Load Trained Models ---\n",
        "# Assume MODEL_SAVE_DIR, MODEL_SAVE_PATH_ENCODER, MODEL_SAVE_PATH_DECODER\n",
        "# EMBED_SIZE, ORIGINAL_FEATURE_DIM, HIDDEN_SIZE, VOCAB_SIZE, NUM_LAYERS, DROPOUT, ATTENTION_DIM, DEVICE\n",
        "# are defined in earlier cells.\n",
        "# Also assume EncoderCNN and DecoderRNN classes are defined.\n",
        "\n",
        "# Example placeholders for undefined variables if not set globally in previous cells\n",
        "# These should ideally come from your global setup (e.g., Cell 1 and Cell 3 definitions)\n",
        "# For the snippet to be runnable, uncomment and set them appropriately:\n",
        "# MODEL_SAVE_DIR = './models'\n",
        "# MODEL_SAVE_PATH_ENCODER = os.path.join(MODEL_SAVE_DIR, 'encoder.pth')\n",
        "# MODEL_SAVE_PATH_DECODER = os.path.join(MODEL_SAVE_DIR, 'decoder.pth')\n",
        "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# START_TOKEN = '<start>'\n",
        "# END_TOKEN = '<end>'\n",
        "# PAD_TOKEN = '<pad>'\n",
        "# UNK_TOKEN = '<unk>'\n",
        "# MAX_SEQ_LEN = 50 # Or your defined max sequence length\n",
        "# BATCH_SIZE = 32 # Or your defined batch size\n",
        "\n",
        "# Dummy definitions for EncoderCNN and DecoderRNN if they are not in scope for testing this cell directly\n",
        "# In your actual notebook, these would be defined in an earlier cell.\n",
        "# class EncoderCNN(torch.nn.Module):\n",
        "#     def __init__(self, embed_size, original_feature_dim):\n",
        "#         super(EncoderCNN, self).__init__()\n",
        "#         self.linear = torch.nn.Linear(original_feature_dim, embed_size)\n",
        "#         self.bn = torch.nn.BatchNorm1d(embed_size)\n",
        "#         self.relu = torch.nn.ReLU()\n",
        "#         self.dropout = torch.nn.Dropout(0.5)\n",
        "#\n",
        "#     def forward(self, features):\n",
        "#         features = self.dropout(self.bn(self.relu(self.linear(features))))\n",
        "#         return features\n",
        "#\n",
        "# class Attention(torch.nn.Module):\n",
        "#     def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "#         super(Attention, self).__init__()\n",
        "#         self.encoder_att = torch.nn.Linear(encoder_dim, attention_dim)\n",
        "#         self.decoder_att = torch.nn.Linear(decoder_dim, attention_dim)\n",
        "#         self.full_att = torch.nn.Linear(attention_dim, 1)\n",
        "#         self.relu = torch.nn.ReLU()\n",
        "#         self.softmax = torch.nn.Softmax(dim=1)\n",
        "#\n",
        "#     def forward(self, encoder_out, decoder_hidden):\n",
        "#         att1 = self.encoder_att(encoder_out) # (batch_size, num_pixels, attention_dim)\n",
        "#         att2 = self.decoder_att(decoder_hidden).unsqueeze(1) # (batch_size, 1, attention_dim)\n",
        "#         combined = self.relu(att1 + att2) # (batch_size, num_pixels, attention_dim)\n",
        "#         attention_score = self.full_att(combined).squeeze(2) # (batch_size, num_pixels)\n",
        "#         alpha = self.softmax(attention_score) # (batch_size, num_pixels)\n",
        "#         context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1) # (batch_size, encoder_dim)\n",
        "#         return context, alpha\n",
        "#\n",
        "# class DecoderRNN(torch.nn.Module):\n",
        "#     def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout, encoder_dim, attention_dim):\n",
        "#         super(DecoderRNN, self).__init__()\n",
        "#         self.embed_size = embed_size\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.vocab_size = vocab_size\n",
        "#         self.num_layers = num_layers\n",
        "#         self.dropout = dropout\n",
        "#         self.encoder_dim = encoder_dim\n",
        "#         self.attention_dim = attention_dim\n",
        "#\n",
        "#         self.embed = torch.nn.Embedding(vocab_size, embed_size)\n",
        "#         self.dropout_embed = torch.nn.Dropout(dropout)\n",
        "#         self.lstm = torch.nn.LSTM(embed_size + encoder_dim, hidden_size, num_layers, batch_first=True) # Input: word embed + context\n",
        "#         self.linear = torch.nn.Linear(hidden_size, vocab_size)\n",
        "#         self.attention = Attention(encoder_dim, hidden_size, attention_dim) # Attention module\n",
        "#         self.feature_to_hidden = torch.nn.Linear(encoder_dim, hidden_size)\n",
        "#         self.feature_to_cell = torch.nn.Linear(encoder_dim, hidden_size)\n",
        "\n",
        "\n",
        "#     def forward(self, features, captions):\n",
        "#         # During training, features are (batch_size, encoder_dim)\n",
        "#         # captions are (batch_size, seq_len)\n",
        "#         embeddings = self.dropout_embed(self.embed(captions)) # (batch_size, seq_len, embed_size)\n",
        "#\n",
        "#         # Initial LSTM states from image features\n",
        "#         h0 = self.feature_to_hidden(features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
        "#         c0 = self.feature_to_cell(features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
        "#\n",
        "#         # Initialize output and attention weights lists\n",
        "#         predictions = torch.zeros(embeddings.size(0), embeddings.size(1), self.vocab_size).to(DEVICE)\n",
        "#         alphas = torch.zeros(embeddings.size(0), embeddings.size(1), 1).to(DEVICE) # Placeholder for attention weights\n",
        "#\n",
        "#         # Loop through the sequence length\n",
        "#         for t in range(embeddings.size(1)):\n",
        "#             # Get context vector using attention\n",
        "#             # features should be (batch_size, feature_dim)\n",
        "#             # h0[-1, :, :] is (batch_size, hidden_size) for the last layer's hidden state\n",
        "#             context_vector, alpha = self.attention(features, h0[-1, :, :])\n",
        "#             alphas[:, t, :] = alpha.mean(dim=1).unsqueeze(1) # Average attention over features for each time step\n",
        "#\n",
        "#             # Concatenate embedded word and context vector\n",
        "#             combined_input = torch.cat((embeddings[:, t, :], context_vector), dim=1).unsqueeze(1) # (batch_size, 1, embed_size + context_dim)\n",
        "#\n",
        "#             # Pass through LSTM\n",
        "#             out, (h0, c0) = self.lstm(combined_input, (h0, c0))\n",
        "#             predictions[:, t, :] = self.linear(out.squeeze(1)) # (batch_size, vocab_size)\n",
        "#\n",
        "#         return predictions\n",
        "#\n",
        "#     def generate_caption(self, features, vocab, idx_to_word_map, max_seq_len, device,\n",
        "#                          strategy='greedy', beam_size=3, temperature=1.0, top_k=None, top_p=None):\n",
        "#         # This method is replaced by the global generate_caption_with_attention function for flexibility\n",
        "#         # But it's good to have a placeholder or remove it if not used\n",
        "#         pass\n",
        "\n",
        "\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    # Check if models are already loaded and are of the correct type\n",
        "    if 'encoder' not in globals() or not isinstance(encoder, torch.nn.Module) or \\\n",
        "       'decoder' not in globals() or not isinstance(decoder, torch.nn.Module):\n",
        "        print(f\"Models not found in memory, loading from {MODEL_SAVE_PATH_ENCODER}...\")\n",
        "        # Re-instantiate models with correct parameters if not in memory\n",
        "        # Make sure EncoderCNN and DecoderRNN classes are available (e.g., defined in Cell 3)\n",
        "        # Ensure ATTENTION_DIM is available here (from Cell 1 or defined)\n",
        "        # ATTENTION_DIM = HIDDEN_SIZE # Example if not global\n",
        "        # These globals must be defined or passed if this cell is run standalone\n",
        "        encoder = EncoderCNN(EMBED_SIZE, ORIGINAL_FEATURE_DIM).to(DEVICE)\n",
        "        decoder = DecoderRNN(EMBED_SIZE, HIDDEN_SIZE, VOCAB_SIZE, NUM_LAYERS, DROPOUT, EMBED_SIZE, ATTENTION_DIM).to(DEVICE)\n",
        "\n",
        "        encoder.load_state_dict(torch.load(MODEL_SAVE_PATH_ENCODER, map_location=DEVICE))\n",
        "        decoder.load_state_dict(torch.load(MODEL_SAVE_PATH_DECODER, map_location=DEVICE))\n",
        "        print(\"Models loaded for evaluation.\")\n",
        "    else:\n",
        "        print(\"Models (encoder and decoder) already found in memory from training.\")\n",
        "\n",
        "    encoder.eval() # Set models to evaluation mode\n",
        "    decoder.eval() # Set models to evaluation mode\n",
        "except FileNotFoundError:\n",
        "    print(f\"WARNING: Model files not found. Please ensure Cell 4 completed training and saved the models to:\")\n",
        "    print(f\"   Encoder: {MODEL_SAVE_PATH_ENCODER}\")\n",
        "    print(f\"   Decoder: {MODEL_SAVE_PATH_DECODER}\")\n",
        "    print(\"Continuing with potentially untrained models if this is the first run after defining them.\")\n",
        "    # If you always want to stop if models aren't found, uncomment the raise:\n",
        "    # raise\n",
        "\n",
        "\n",
        "# --- MODIFIED: Caption Generation Function with Attention and Sampling Options ---\n",
        "\n",
        "\n",
        "def generate_caption_with_attention(model, features, vocab, idx_to_word_map, max_seq_len, device,\n",
        "                                    strategy='greedy', beam_size=3, temperature=1.0, top_k=None, top_p=None):\n",
        "    \"\"\"\n",
        "    Generate a caption using attention mechanism with support for decoding strategies:\n",
        "    greedy, beam search, top-k sampling, and top-p (nucleus) sampling.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Initial states from image features. features should be (1, D)\n",
        "        # Squeeze features to ensure it's (1, D) if it somehow got (1, 1, D)\n",
        "        features_for_init = features.squeeze(1) if features.dim() == 3 else features\n",
        "        h_t = model.feature_to_hidden(features_for_init).unsqueeze(0).repeat(model.num_layers, 1, 1)\n",
        "        c_t = model.feature_to_cell(features_for_init).unsqueeze(0).repeat(model.num_layers, 1, 1)\n",
        "        states = (h_t, c_t)\n",
        "\n",
        "        if strategy == 'beam_search' and beam_size > 1:\n",
        "            # Ensure the initial word ID tensor is (1,)\n",
        "            sequences = [[torch.tensor([vocab[START_TOKEN]], dtype=torch.long).to(device).view(1), 0.0, states]]\n",
        "\n",
        "            for _ in range(max_seq_len):\n",
        "                all_candidates = []\n",
        "                finished_sequences_in_step = [] # To store sequences that have ended\n",
        "                for seq, score, current_states in sequences:\n",
        "                    last_word_id = seq[-1].item()\n",
        "                    if last_word_id == vocab[END_TOKEN]:\n",
        "                        finished_sequences_in_step.append((seq, score, current_states))\n",
        "                        continue # This sequence is done, move to next in current beam\n",
        "\n",
        "                    current_hidden = current_states[0][-1, :, :] # (1, hidden_size)\n",
        "\n",
        "                    # Ensure features for attention are (1, feature_dim)\n",
        "                    norm_features = features.squeeze(1) if features.dim() == 3 else features\n",
        "                    context_vector, _ = model.attention(norm_features, current_hidden) # Should be (1, attention_dim)\n",
        "\n",
        "                    # Ensure embedded_word is (1, 1, embed_size)\n",
        "                    embedded_word = model.dropout_embed(model.embed(torch.tensor([last_word_id], dtype=torch.long).to(device).view(1))).unsqueeze(1) # (1, 1, embed_size)\n",
        "\n",
        "                    # Ensure context_vector is (1, 1, attention_dim)\n",
        "                    if context_vector.dim() == 2:\n",
        "                        context_vector = context_vector.unsqueeze(1) # Transform (1, attention_dim) to (1, 1, attention_dim)\n",
        "                    elif context_vector.dim() == 1: # Fallback for unexpected 1D tensor\n",
        "                        context_vector = context_vector.unsqueeze(0).unsqueeze(0) # Transform (attention_dim,) to (1, 1, attention_dim)\n",
        "                    # If already (1,1,D), do nothing. If >3D, there's a problem outside this function.\n",
        "\n",
        "                    combined_input = torch.cat((embedded_word, context_vector), dim=2) # Both are (1, 1, X) -> Result (1, 1, embed + context)\n",
        "                    output, new_states = model.lstm(combined_input, current_states)\n",
        "                    logits = model.linear(output.squeeze(1)) # (1, vocab_size)\n",
        "                    log_probs = F.log_softmax(logits, dim=-1)\n",
        "                    top_log_probs, top_indices = log_probs.topk(beam_size, dim=1)\n",
        "\n",
        "                    for i in range(beam_size):\n",
        "                        new_seq = torch.cat([seq, torch.tensor([top_indices[0][i]], dtype=torch.long).to(device).view(1)])\n",
        "                        new_score = score + top_log_probs[0][i].item()\n",
        "                        all_candidates.append((new_seq, new_score, new_states))\n",
        "\n",
        "                # Combine finished and active candidates, then sort and prune\n",
        "                sequences = sorted(finished_sequences_in_step + all_candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "                # If the top sequence in the beam has ended, and we aren't forcing max_seq_len, we can stop early\n",
        "                if sequences and sequences[0][0][-1].item() == vocab[END_TOKEN]:\n",
        "                    break\n",
        "\n",
        "            generated_ids = sequences[0][0].tolist() # Get the word IDs from the best sequence\n",
        "\n",
        "        else: # Greedy, Top-K, Top-P sampling\n",
        "            generated_ids = []\n",
        "            # Ensure input_word is always (1,)\n",
        "            input_word = torch.tensor([vocab[START_TOKEN]], dtype=torch.long).to(device).view(1)\n",
        "            hidden_state, cell_state = states\n",
        "            for _ in range(max_seq_len):\n",
        "                current_hidden = hidden_state[-1, :, :] # (1, hidden_size)\n",
        "\n",
        "                # Ensure features for attention are (1, feature_dim)\n",
        "                norm_features = features.squeeze(1) if features.dim() == 3 else features\n",
        "                context_vector, _ = model.attention(norm_features, current_hidden) # Should be (1, attention_dim)\n",
        "\n",
        "                # Ensure embedded_word is (1, 1, embed_size)\n",
        "                embedded_word = model.dropout_embed(model.embed(input_word)).unsqueeze(1) # (1, 1, embed_size)\n",
        "\n",
        "                # Ensure context_vector is (1, 1, attention_dim)\n",
        "                if context_vector.dim() == 2:\n",
        "                    context_vector = context_vector.unsqueeze(1) # Transform (1, attention_dim) to (1, 1, attention_dim)\n",
        "                elif context_vector.dim() == 1: # Fallback for unexpected 1D tensor\n",
        "                    context_vector = context_vector.unsqueeze(0).unsqueeze(0) # Transform (attention_dim,) to (1, 1, attention_dim)\n",
        "                # If already (1,1,D), do nothing. If >3D, there's a problem outside this function.\n",
        "\n",
        "                combined_input = torch.cat((embedded_word, context_vector), dim=2) # Both are (1, 1, X) -> Result (1, 1, embed + context)\n",
        "                output, (hidden_state, cell_state) = model.lstm(combined_input, (hidden_state, cell_state))\n",
        "                logits = model.linear(output.squeeze(1)) # (1, vocab_size)\n",
        "\n",
        "                if strategy == 'greedy':\n",
        "                    predicted_id = torch.argmax(logits, dim=1)\n",
        "\n",
        "                elif strategy == 'top-k':\n",
        "                    logits = logits / temperature\n",
        "                    top_logits, top_indices = logits.topk(top_k, dim=-1)\n",
        "                    probs = F.softmax(top_logits, dim=-1)\n",
        "                    sampled = torch.multinomial(probs, 1)\n",
        "                    predicted_id = top_indices.gather(-1, sampled)\n",
        "\n",
        "                elif strategy == 'top-p':\n",
        "                    logits = logits / temperature\n",
        "                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "                    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                    logits[:, indices_to_remove] = -float(\"Inf\")\n",
        "                    probs = F.softmax(logits, dim=-1)\n",
        "                    predicted_id = torch.multinomial(probs, 1)\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown decoding strategy: {strategy}\")\n",
        "\n",
        "                predicted_id_item = predicted_id.item()\n",
        "                if predicted_id_item == vocab[END_TOKEN]:\n",
        "                    break\n",
        "                generated_ids.append(predicted_id_item)\n",
        "                input_word = predicted_id.detach().view(1) # Ensure next input_word is also (1,)\n",
        "\n",
        "        generated_words = [idx_to_word_map[idx] for idx in generated_ids]\n",
        "        generated_words = [w for w in generated_words if w not in [START_TOKEN, END_TOKEN]]\n",
        "        return ' '.join(generated_words) if generated_words else UNK_TOKEN\n",
        "\n",
        "\n",
        "# --- 2. Evaluate on Test Data and Generate Sample Captions ---\n",
        "reference_captions = []\n",
        "candidate_captions = []\n",
        "sample_image_data = [] # Stores dicts with Image_ID, Original_Report, Generated_Caption for display\n",
        "meteor_scores = []\n",
        "\n",
        "print(\"\\nGenerating captions for test set and collecting metrics...\")\n",
        "with torch.no_grad():\n",
        "    for i, (image_feats, report_seqs, lengths) in enumerate(test_dataloader):\n",
        "        if i >= 100: # Evaluate on first 100 batches (adjust as needed for full eval)\n",
        "            print(f\"Limiting evaluation to first {i} batches for demonstration.\")\n",
        "            break\n",
        "\n",
        "        batch_size = image_feats.shape[0]\n",
        "\n",
        "        for j in range(batch_size):\n",
        "            img_feat = image_feats[j] # This is (original_feature_dim,)\n",
        "\n",
        "            # Get the ground truth report for comparison\n",
        "            true_report_indices = report_seqs[j].tolist()\n",
        "            true_report_words = [\n",
        "                idx_to_word[idx] for idx in true_report_indices\n",
        "                if idx not in [vocab[PAD_TOKEN], vocab[START_TOKEN], vocab[END_TOKEN]]\n",
        "            ]\n",
        "            true_report_clean = ' '.join(true_report_words)\n",
        "\n",
        "            # --- Generate Caption using the updated function ---\n",
        "            # For metric calculation, it's common to use beam search (less stochastic)\n",
        "            # You can experiment with different beam sizes.\n",
        "            # encoded_features from encoder is (1, embed_size)\n",
        "            encoded_features = encoder(img_feat.unsqueeze(0).to(DEVICE)).squeeze(0) # Remove batch dim (1,)\n",
        "\n",
        "            # Use beam search for metrics (set beam_size > 1, strategy='beam_search')\n",
        "            generated_caption_text_for_metrics = generate_caption_with_attention(\n",
        "                decoder, encoded_features, vocab, idx_to_word, MAX_SEQ_LEN, DEVICE,\n",
        "                strategy='beam_search', beam_size=3 # Adjust beam_size as needed\n",
        "            )\n",
        "\n",
        "            # For sample display, use a stochastic method to encourage diversity\n",
        "            # Adjust top_p and temperature for desired randomness.\n",
        "            # Using same function but with different strategy and params.\n",
        "            generated_caption_text_for_display = generate_caption_with_attention(\n",
        "                decoder, encoded_features, vocab, idx_to_word, MAX_SEQ_LEN, DEVICE,\n",
        "                strategy='top-p', top_p=0.9, temperature=0.7 # Example values for top-p sampling\n",
        "            )\n",
        "\n",
        "\n",
        "            # Ensure non-empty candidates for metric calculation\n",
        "            if not generated_caption_text_for_metrics.strip():\n",
        "                generated_caption_text_for_metrics = UNK_TOKEN # Fallback for metrics\n",
        "\n",
        "            reference_captions.append([nltk.word_tokenize(true_report_clean.lower())])\n",
        "            candidate_captions.append(nltk.word_tokenize(generated_caption_text_for_metrics.lower()))\n",
        "\n",
        "            current_ref_meteor_tokens = [nltk.word_tokenize(true_report_clean.lower())]\n",
        "            current_cand_meteor_tokens = nltk.word_tokenize(generated_caption_text_for_metrics.lower())\n",
        "            meteor_scores.append(meteor_score(current_ref_meteor_tokens, current_cand_meteor_tokens))\n",
        "\n",
        "\n",
        "            # Get the original row from the test_df using its index\n",
        "            original_df_row_index = (i * BATCH_SIZE) + j\n",
        "            # Make sure test_df is accessible and populated (e.g., loaded in an earlier cell)\n",
        "            if 'test_df' in globals() and original_df_row_index < len(test_df):\n",
        "                full_image_id_from_df = str(test_df.iloc[original_df_row_index]['Person_id'])\n",
        "                current_original_report_full = test_df.iloc[original_df_row_index]['Report']\n",
        "            else:\n",
        "                full_image_id_from_df = f\"Image_{original_df_row_index}\" # Dummy ID if test_df not available\n",
        "                current_original_report_full = \"Original Report N/A (test_df not loaded or index out of bounds)\"\n",
        "\n",
        "            # *** UPDATED: Adjust sampling rate and total samples ***\n",
        "            # Increased probability to 20% and max samples to 10 for better diversity\n",
        "            # Use the more diverse caption for display\n",
        "            if random.random() < 0.2 and len(sample_image_data) < 10:\n",
        "                sample_image_data.append({\n",
        "                    'Image_ID_Path': full_image_id_from_df,\n",
        "                    'Original_Report': current_original_report_full,\n",
        "                    'Generated_Caption': generated_caption_text_for_display # Use the stochastically generated one\n",
        "                })\n",
        "\n",
        "\n",
        "# --- 3. Calculate BLEU Scores ---\n",
        "print(\"\\nCalculating BLEU scores...\")\n",
        "smooth = SmoothingFunction().method1\n",
        "bleu1_scores = []\n",
        "bleu2_scores = []\n",
        "bleu3_scores = []\n",
        "bleu4_scores = []\n",
        "\n",
        "for ref, cand in zip(reference_captions, candidate_captions):\n",
        "    if not cand:\n",
        "        cand = [UNK_TOKEN] # or ['<unk>'] for better placeholder handling\n",
        "    bleu1_scores.append(sentence_bleu(ref, cand, weights=(1, 0, 0, 0), smoothing_function=smooth))\n",
        "    bleu2_scores.append(sentence_bleu(ref, cand, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth))\n",
        "    bleu3_scores.append(sentence_bleu(ref, cand, weights=(0.33, 0.33, 0.34, 0), smoothing_function=smooth))\n",
        "    bleu4_scores.append(sentence_bleu(ref, cand, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth))\n",
        "\n",
        "avg_bleu1 = np.mean(bleu1_scores) if bleu1_scores else 0\n",
        "avg_bleu2 = np.mean(bleu2_scores) if bleu2_scores else 0\n",
        "avg_bleu3 = np.mean(bleu3_scores) if bleu3_scores else 0\n",
        "avg_bleu4 = np.mean(bleu4_scores) if bleu4_scores else 0\n",
        "\n",
        "print(f\"Average BLEU-1 Score: {avg_bleu1:.4f}\")\n",
        "print(f\"Average BLEU-2 Score: {avg_bleu2:.4f}\")\n",
        "print(f\"Average BLEU-3 Score: {avg_bleu3:.4f}\")\n",
        "print(f\"Average BLEU-4 Score: {avg_bleu4:.4f}\")\n",
        "\n",
        "\n",
        "# --- 4. Calculate ROUGE Scores ---\n",
        "print(\"\\nCalculating ROUGE scores...\")\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougeL_scores = []\n",
        "\n",
        "for ref_list, cand_list in zip(reference_captions, candidate_captions):\n",
        "    reference_string = ' '.join(ref_list[0])\n",
        "    candidate_string = ' '.join(cand_list)\n",
        "    if not candidate_string.strip():\n",
        "        candidate_string = UNK_TOKEN # Fallback for ROUGE\n",
        "    scores = scorer.score(reference_string, candidate_string)\n",
        "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "avg_rouge1 = np.mean(rouge1_scores) if rouge1_scores else 0\n",
        "avg_rouge2 = np.mean(rouge2_scores) if rouge2_scores else 0\n",
        "avg_rougeL = np.mean(rougeL_scores) if rougeL_scores else 0\n",
        "\n",
        "print(f\"Average ROUGE-1 F-measure: {avg_rouge1:.4f}\")\n",
        "print(f\"Average ROUGE-2 F-measure: {avg_rouge2:.4f}\")\n",
        "print(f\"Average ROUGE-L F-measure: {avg_rougeL:.4f}\")\n",
        "\n",
        "\n",
        "# --- 5. Calculate METEOR Score ---\n",
        "print(\"\\nCalculating METEOR score...\")\n",
        "avg_meteor = np.mean(meteor_scores) if meteor_scores else 0\n",
        "print(f\"Average METEOR Score: {avg_meteor:.4f}\")\n",
        "\n",
        "\n",
        "# --- 6. Calculate CIDEr Score ---\n",
        "print(\"\\nCalculating CIDEr score :\")\n",
        "\n",
        "# Prepare COCO-style dicts\n",
        "gts_dict = {}  # Ground Truths\n",
        "res_dict = {}  # Predictions\n",
        "\n",
        "for i, (ref_list, cand_list) in enumerate(zip(reference_captions, candidate_captions)):\n",
        "    image_id = str(i)\n",
        "    gts_dict[image_id] = [{'caption': ' '.join(ref_list[0])}]\n",
        "    res_dict[image_id] = [{'caption': ' '.join(cand_list)}]\n",
        "\n",
        "# Tokenize captions as expected by Cider scorer\n",
        "# This implicitly handles the Stanford CoreNLP download if not present.\n",
        "# If it fails here, you have the connection issue.\n",
        "try:\n",
        "    tokenizer = PTBTokenizer()\n",
        "    gts_tokenized = tokenizer.tokenize(gts_dict)\n",
        "    res_tokenized = tokenizer.tokenize(res_dict)\n",
        "\n",
        "    # Compute CIDEr score\n",
        "    cider_scorer = Cider()\n",
        "    cider_score, _ = cider_scorer.compute_score(gts_tokenized, res_tokenized)\n",
        "\n",
        "    print(f\"CIDEr Score: {cider_score:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not calculate CIDEr score. Error: {e}\")\n",
        "    print(\"This often happens if the Stanford CoreNLP library for PTBTokenizer cannot be downloaded.\")\n",
        "    print(\"Please ensure your internet connection is stable or manually install Stanford CoreNLP.\")\n",
        "\n",
        "\n",
        "# --- 7. Display Sample Predictions ---\n",
        "# This section remains largely the same, ensuring it's at the end.\n",
        "print(\"\\n--- Sample Generated Captions ---\")\n",
        "if not sample_image_data:\n",
        "    print(\"No sample captions collected. Try adjusting the random.random() threshold or batch limit.\")\n",
        "\n",
        "for i, sample in enumerate(sample_image_data):\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    print(f\"Image ID in DataFrame: {sample['Image_ID_Path']}\")\n",
        "    print(f\"Original Report: {sample['Original_Report']}\")\n",
        "    print(f\"Generated Caption: {sample['Generated_Caption']}\")\n",
        "\n",
        "\n",
        "print(\"\\nModel evaluation and inference complete.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}