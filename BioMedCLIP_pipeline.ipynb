{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 31906,
     "status": "ok",
     "timestamp": 1763640870099,
     "user": {
      "displayName": "Ravipati Rishitha",
      "userId": "02972511922802629867"
     },
     "user_tz": -330
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install Libraries\n",
    "!pip install open_clip_torch==2.23.0 transformers==4.35.2 matplotlib -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install nltk -q # Ensure nltk is installed for text processing and metrics\n",
    "!pip install rouge -q # Ensure rouge is installed for ROUGE score\n",
    "!pip install evaluate -q # A HuggingFace library for easier metric calculation\n",
    "\n",
    "# For METEOR score, often requires specific installation or a separate script\n",
    "# if meteor_score is not part of the 'evaluate' library or nltk directly.\n",
    "# If you used 'meteor_score.py' previously, ensure it's accessible.\n",
    "# For consistency, it's good to use a unified metrics library if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 66922,
     "status": "ok",
     "timestamp": 1763640937025,
     "user": {
      "displayName": "Ravipati Rishitha",
      "userId": "02972511922802629867"
     },
     "user_tz": -330
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Running on device: cuda\n",
      "CUDA Device: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Mount Google Drive and Define Global Configuration\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import open_clip # from open_clip_torch\n",
    "import numpy as np\n",
    "import torch.cuda.amp as amp # <--- ADD THIS LINE\n",
    "\n",
    "# Mount your drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Running on device: {DEVICE}\")\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available, running on CPU. This will be slower.\")\n",
    "\n",
    "# Paths to your CSV and images in Google Drive\n",
    "# In Cell 1, update these paths\n",
    "INPUT_CSV_FILES = [\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/Final_Train_Data.csv\", # <--- CHANGE THIS LINE\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/Final_CV_Data.csv\",    # <--- CHANGE THIS LINE\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/Final_Test_Data.csv\"    # <--- CHANGE THIS LINE\n",
    "]\n",
    "IMAGE_BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks /NLMCXR_png\" # <--- ALSO UPDATE THIS IF NLMCXR_png is there\n",
    "# --- CHECK THIS PATH CAREFULLY AGAINST YOUR GOOGLE DRIVE ---\n",
    "# <--- Adjusted based on common setup/screenshot\n",
    "# --- END PATH CHECK ---\n",
    "\n",
    "# --- IMPORTANT: Set the model identifier for output filenames ---\n",
    "# Based on your files, we will use the BioMedCLIP model locally.\n",
    "MODEL_IDENTIFIER_FOR_OUTPUT = \"biomedclip_vitb16\"\n",
    "\n",
    "# This variable needs to be consistently defined for Cell 3\n",
    "BIOMEDCLIP_LOCAL_MODEL_DIR = \"/content/drive/MyDrive/BiomedCLIP_Model_Files\" # <--- Ensure this is correct\n",
    "os.makedirs(BIOMEDCLIP_LOCAL_MODEL_DIR, exist_ok=True) # Ensure it exists if you upload manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2393,
     "status": "ok",
     "timestamp": 1763640939433,
     "user": {
      "displayName": "Ravipati Rishitha",
      "userId": "02972511922802629867"
     },
     "user_tz": -330
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading BioMedCLIP model from local directory: /content/drive/MyDrive/BiomedCLIP_Model_Files using open_clip...\n",
      "Model (visual part) converted to torch.float16 for CUDA acceleration.\n",
      "BioMedCLIP model and preprocessor loaded successfully from local drive!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load the BioMedCLIP Model\n",
    "\n",
    "# Ensure BIOMEDCLIP_LOCAL_MODEL_DIR is set (it should be from Cell 2 now)\n",
    "if 'BIOMEDCLIP_LOCAL_MODEL_DIR' not in locals():\n",
    "    # Fallback if cell 2 was skipped, but ideally set there.\n",
    "    BIOMEDCLIP_LOCAL_MODEL_DIR = \"/content/drive/MyDrive/BiomedCLIP_Model_Files\"\n",
    "\n",
    "print(f\"\\nLoading BioMedCLIP model from local directory: {BIOMEDCLIP_LOCAL_MODEL_DIR} using open_clip...\")\n",
    "try:\n",
    "    # Use torch.float16 for `precision` if DEVICE is 'cuda', otherwise float32.\n",
    "    # This ensures consistency with the model.half() call below.\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        model_name=\"ViT-B-16\",\n",
    "        pretrained=None, # Important: Set to None as you're loading from local cache_dir\n",
    "        cache_dir=BIOMEDCLIP_LOCAL_MODEL_DIR,\n",
    "        precision='fp16' if DEVICE == 'cuda' else 'fp32', # Use the global DEVICE variable\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    # --- IMPORTANT: Convert model to half precision (fp16) for GPU if not already ---\n",
    "    # open_clip's create_model_and_transforms' `precision` argument primarily controls how weights are loaded.\n",
    "    # `model.half()` explicitly converts the *entire model* to FP16, which is crucial for performance on GPUs.\n",
    "    if DEVICE == 'cuda':\n",
    "        model = model.half() # Convert model to half precision (fp16)\n",
    "        # Check the dtype of a specific parameter if you want to verify\n",
    "        if hasattr(model, 'visual') and hasattr(model.visual, 'conv1') and hasattr(model.visual.conv1, 'weight'):\n",
    "            print(f\"Model (visual part) converted to {model.visual.conv1.weight.dtype} for CUDA acceleration.\")\n",
    "        else:\n",
    "            print(\"Model converted to half precision (fp16) for CUDA acceleration.\")\n",
    "    # --- END IMPORTANT CHANGE ---\n",
    "\n",
    "    tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "    model.eval() # Set model to evaluation mode (disables dropout, batch norm updates)\n",
    "    print(\"BioMedCLIP model and preprocessor loaded successfully from local drive!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading BioMedCLIP with open_clip from local drive: {e}\")\n",
    "    print(f\"Please ensure the directory '{BIOMEDCLIP_LOCAL_MODEL_DIR}' exists and contains all required files (e.g., config.json, pytorch_model.bin).\")\n",
    "    raise # Re-raise the exception to stop execution if model loading fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1763640939441,
     "user": {
      "displayName": "Ravipati Rishitha",
      "userId": "02972511922802629867"
     },
     "user_tz": -330
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Define Feature Extraction Function\n",
    "\n",
    "# -----------------------------\n",
    "# Feature Extraction Function\n",
    "# -----------------------------\n",
    "def extract_features(image_path, model, preprocess, device):\n",
    "    try:\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Warning: Image file not found at {image_path}. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Preprocess the image using open_clip's preprocess function\n",
    "        # preprocess() typically outputs float32, which is fine for autocast\n",
    "        image_input = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "        # --- NEW CHANGE HERE: Use autocast for mixed precision inference ---\n",
    "        with torch.no_grad():\n",
    "           if device == 'cuda':\n",
    "                with torch.amp.autocast(device_type='cuda'): # Recommended syntax\n",
    "                    image_features = model.encode_image(image_input)\n",
    "           else:\n",
    "                image_features = model.encode_image(image_input)\n",
    "\n",
    "        # Convert to float32 and move to CPU, then to numpy\n",
    "        return image_features.float().cpu().numpy().squeeze()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting feature extraction process for all CSV files...\n",
      "\n",
      "Processing /content/drive/MyDrive/Colab Notebooks/Final_Train_Data.csv with 2764 rows...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df457353f7a346d3b517dcd505cd0b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7e7ec7992043439e99f62425a8db86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /content/drive/MyDrive/Colab Notebooks/Final_Train_Data_with_biomedclip_vitb16_features.pkl\n",
      "\n",
      "Processing /content/drive/MyDrive/Colab Notebooks/Final_CV_Data.csv with 609 rows...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74ccf3ca7ad49d898cb57cee1c68f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/609 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988c1180237c4fb8ab3649d8bd38ce9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/609 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /content/drive/MyDrive/Colab Notebooks/Final_CV_Data_with_biomedclip_vitb16_features.pkl\n",
      "\n",
      "Processing /content/drive/MyDrive/Colab Notebooks/Final_Test_Data.csv with 377 rows...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244b63c530d84993aa10a3eabd7407a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe47a8c243844dba9aa5e0f9f6627945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title 5. Process CSV Files and Save Features\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f\"\\nStarting feature extraction process for all CSV files...\")\n",
    "\n",
    "for csv_path in INPUT_CSV_FILES:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\nProcessing {csv_path} with {len(df)} rows...\")\n",
    "\n",
    "    if 'Image1' not in df.columns or 'Image2' not in df.columns:\n",
    "        print(f\"Warning: Missing 'Image1' or 'Image2' columns in {csv_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    df['image_feat_img1'] = df['Image1'].progress_apply(\n",
    "        lambda x: extract_features(\n",
    "            os.path.join(IMAGE_BASE_DIR, x.replace('NLMCXR_png/', '', 1) if x and x.startswith('NLMCXR_png/') else x), # Added 'if x' for robustness\n",
    "            model, preprocess, DEVICE\n",
    "        ) if pd.notna(x) else None\n",
    "    )\n",
    "    df['image_feat_img2'] = df['Image2'].progress_apply(\n",
    "        lambda x: extract_features(\n",
    "            os.path.join(IMAGE_BASE_DIR, x.replace('NLMCXR_png/', '', 1) if x and x.startswith('NLMCXR_png/') else x), # Added 'if x' for robustness\n",
    "            model, preprocess, DEVICE\n",
    "        ) if pd.notna(x) else None\n",
    "    )\n",
    "\n",
    "    output_path = csv_path.replace(\".csv\", f\"_with_{MODEL_IDENTIFIER_FOR_OUTPUT}_features.pkl\")\n",
    "    df.to_pickle(output_path)\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "print(f\"\\nâœ… All done. {MODEL_IDENTIFIER_FOR_OUTPUT} features saved.\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Verification Block: Loading and checking the saved features\n",
    "# ğŸš¨ğŸš¨ğŸš¨ CRITICAL: FIX THESE PATHS TO MATCH YOUR SAVED FILES ğŸš¨ğŸš¨ğŸš¨\n",
    "# ----------------------------------------------------\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Verifying Loaded Processed Data ---\")\n",
    "\n",
    "# These paths MUST match the output filenames generated by the loop above.\n",
    "# They are derived from INPUT_CSV_FILES and MODEL_IDENTIFIER_FOR_OUTPUT.\n",
    "base_dir_of_csvs = os.path.dirname(INPUT_CSV_FILES[0]) # Assuming all CSVs are in the same folder\n",
    "\n",
    "TRAIN_PKL_PATH = os.path.join(base_dir_of_csvs, f\"Final_Train_Data_with_{MODEL_IDENTIFIER_FOR_OUTPUT}_features.pkl\")\n",
    "CV_PKL_PATH = os.path.join(base_dir_of_csvs, f\"Final_CV_Data_with_{MODEL_IDENTIFIER_FOR_OUTPUT}_features.pkl\")\n",
    "TEST_PKL_PATH = os.path.join(base_dir_of_csvs, f\"Final_Test_Data_with_{MODEL_IDENTIFIER_FOR_OUTPUT}_features.pkl\")\n",
    "\n",
    "print(f\"Attempting to load from: \\nTrain: {TRAIN_PKL_PATH}\\nCV: {CV_PKL_PATH}\\nTest: {TEST_PKL_PATH}\")\n",
    "\n",
    "try:\n",
    "    df_train_features = pd.read_pickle(TRAIN_PKL_PATH)\n",
    "    print(f\"\\nLoaded training data with features from: {TRAIN_PKL_PATH}\")\n",
    "    print(f\"Shape of loaded training data: {df_train_features.shape}\")\n",
    "    print(f\"Columns in training data: {df_train_features.columns.tolist()}\")\n",
    "\n",
    "    df_cv_features = pd.read_pickle(CV_PKL_PATH)\n",
    "    print(f\"\\nLoaded CV data with features from: {CV_PKL_PATH}\")\n",
    "    print(f\"Shape of loaded CV data: {df_cv_features.shape}\")\n",
    "    print(f\"Columns in CV data: {df_cv_features.columns.tolist()}\")\n",
    "\n",
    "    df_test_features = pd.read_pickle(TEST_PKL_PATH)\n",
    "    print(f\"\\nLoaded test data with features from: {TEST_PKL_PATH}\")\n",
    "    print(f\"Shape of loaded test data: {df_test_features.shape}\")\n",
    "    print(f\"Columns in test data: {df_test_features.columns.tolist()}\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows of training data with features:\")\n",
    "    print(df_train_features[['Image1', 'image_feat_img1', 'Image2', 'image_feat_img2']].head())\n",
    "\n",
    "    print(\"\\nChecking for any None values in feature columns:\")\n",
    "    print(f\"None values in 'image_feat_img1' (Train): {df_train_features['image_feat_img1'].isnull().sum()}\")\n",
    "    print(f\"None values in 'image_feat_img2' (Train): {df_train_features['image_feat_img2'].isnull().sum()}\")\n",
    "    print(f\"None values in 'image_feat_img1' (CV): {df_cv_features['image_feat_img1'].isnull().sum()}\")\n",
    "    print(f\"None values in 'image_feat_img2' (CV): {df_cv_features['image_feat_img2'].isnull().sum()}\")\n",
    "    print(f\"None values in 'image_feat_img1' (Test): {df_test_features['image_feat_img1'].isnull().sum()}\")\n",
    "    print(f\"None values in 'image_feat_img2' (Test): {df_test_features['image_feat_img2'].isnull().sum()}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ğŸš« Error: A .pkl file was not found. Please ensure all paths in Cell 2 are correct and Cell 5 successfully completed the feature extraction process.\")\n",
    "    print(f\"Details: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš« An unexpected error occurred while loading or processing the pickle files: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "path = \"/content/drive/MyDrive/Colab Notebooks /BioMedCLIP_pipeline.ipynb\"\n",
    "\n",
    "nb = nbformat.read(path, as_version=4)\n",
    "\n",
    "# Remove global widget metadata\n",
    "if \"widgets\" in nb[\"metadata\"]:\n",
    "    del nb[\"metadata\"][\"widgets\"]\n",
    "\n",
    "# Clean metadata of each cell\n",
    "for cell in nb[\"cells\"]:\n",
    "    if \"metadata\" in cell:\n",
    "        # Remove widget metadata\n",
    "        if \"widgets\" in cell[\"metadata\"]:\n",
    "            del cell[\"metadata\"][\"widgets\"]\n",
    "\n",
    "        # Remove unnecessary Colab metadata that breaks GitHub rendering\n",
    "        for key in [\"colab\", \"id\", \"outputId\", \"execution\"]:\n",
    "            if key in cell[\"metadata\"]:\n",
    "                del cell[\"metadata\"][key]\n",
    "\n",
    "        # Clean outputs metadata too\n",
    "        if \"outputs\" in cell:\n",
    "            for output in cell[\"outputs\"]:\n",
    "                if hasattr(output, \"metadata\") and \"widgets\" in output.metadata:\n",
    "                    del output.metadata[\"widgets\"]\n",
    "\n",
    "# Write cleaned notebook\n",
    "nbformat.write(nb, path)\n",
    "\n",
    "print(\"Deep-clean complete. All widget metadata removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"/content/drive/MyDrive/Colab Notebooks /BioMedCLIP_pipeline.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "18kX9d7YOqA_s8SMerCaA0b5VtIzACjxq",
     "timestamp": 1751468430201
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
